[
  
  {
    "title": "I've Noticed AI Tools Generate Terrible REST APIs - Here's How to Fix It",
    "url": "/posts/ive-noticed-ai-tools-generate-terrible-rest-apis/",
    "categories": "Posts, REST, API designing, Software Engineering, AI Assisted coding",
    "tags": "posts, api, restful, software, ai",
    "date": "2025-07-10 00:00:00 +0530",
    





    
    "snippet": "Here‚Äôs something that bugs me: AI tools like ChatGPT, Claude, Gemini, or whatever AI you‚Äôre using are great at cranking out code fast,but they often generate REST APIs that look like they were desi...",
    "content": "Here‚Äôs something that bugs me: AI tools like ChatGPT, Claude, Gemini, or whatever AI you‚Äôre using are great at cranking out code fast,but they often generate REST APIs that look like they were designed by someone who never had to maintain them.Over the past few years working with different teams, I‚Äôve noticed that about most of AI generated APIs have naming conventions that make me question everything.Why does this happen? AI models learn from existing code on the internet, and unfortunately, there‚Äôs a lot of bad API design out there. When you ask an AI to ‚Äúcreate a book API,‚Äù it might give you something like this:GET https://example.com/api/books/get-one/{book-id}GET https://example.com/api/books/get-allGET https://example.com/api/books/filter-by-author-id/authorId=1&amp;autherId=2&amp;autherId=3...POST https://example.com/api/create-bookPUT https://example.com/api/update-bookIf your APIs look like this, don‚Äôt take it personally. This post isn‚Äôt meant to shame anyone. The problem is that AI tools often copy patterns from Stack Overflow answers, old tutorials, and legacy codebases where people were just trying to get something working quickly.But here‚Äôs the thing: APIs are interfaces that other developers (including future you) will have to work with. A poorly designed API can waste hours of development time, create confusion in documentation, and make your codebase harder to maintain.Let me show you why these examples are problematic and how to fix them.Too Bored to Read? Copy This PromptIf you‚Äôre too bored to read this entire post, just copy and paste this comprehensive prompt into your AI coding assistant so it follows these guidelines when generating REST APIs. Instructions for each tool are included below:      üìã Click to expand comprehensive AI prompt for REST API guidelines        Copy    You are a professional AI coding assistant. Follow these REST API design rules religiously when generating any REST API related code, endpoints, or architecture.## MANDATORY REST API RULES### URL DESIGN- Use nouns, NEVER verbs in URLs- Collections MUST be plural: /api/v1/books not /api/v1/book- Always version APIs: /api/v1/, /api/v2/- Structure: {protocol}://{host}/api/{version}/{collection}/{id}?{query_params}### HTTP METHODS- GET: Read operations only (safe, idempotent, no side effects)- POST: Create resources or complex operations- PUT: Replace entire resource (send all fields)- PATCH: Update partial resource (send only changed fields)- DELETE: Remove resource### RESPONSE STANDARDS- Always include proper HTTP status codes (200, 201, 400, 401, 403, 404, 500)- Always include pagination for collections: ?page=1&amp;limit=20- Use consistent naming convention (pick snake_case OR camelCase, never mix)- Structure error responses with field-level details### CORRECT PATTERNSGET    /api/v1/books                    # List booksGET    /api/v1/books/123               # Get specific bookPOST   /api/v1/books                   # Create bookPUT    /api/v1/books/123               # Update entire bookPATCH  /api/v1/books/123               # Update book partiallyDELETE /api/v1/books/123               # Delete bookGET    /api/v1/authors/456/books       # Author's books (hierarchical)POST   /api/v1/authors/456/books       # Create book for authorGET    /api/v1/books?author=tolkien&amp;genre=fantasy    # Simple filteringPOST   /api/v1/books/search            # Complex filtering (use POST)### FORBIDDEN PATTERNS - NEVER GENERATE THESE‚ùå GET /api/books/get-one/123‚ùå GET /api/books/get-all‚ùå POST /api/create-book‚ùå PUT /api/update-book‚ùå GET /api/getBookById/123‚ùå POST /api/createBook‚ùå DELETE /api/removeBook/123### REQUIRED RESPONSE FORMATSSuccess Response:{  \"data\": { \"id\": 123, \"title\": \"The Hobbit\" }}Collection with Pagination:{  \"data\": [],  \"pagination\": {    \"page\": 1,    \"limit\": 20,    \"total\": 150,    \"totalPages\": 8  }}Error Response:{  \"error\": {    \"code\": \"VALIDATION_ERROR\",    \"message\": \"Invalid request data\",    \"details\": [      { \"field\": \"price\", \"message\": \"Must be positive number\" }    ]  }}### CHECKLIST - VERIFY BEFORE GENERATING- [ ] URLs use nouns, not verbs- [ ] Collections are plural- [ ] Consistent naming throughout- [ ] Proper HTTP methods- [ ] API versioning (/api/v1/)- [ ] Pagination for collections- [ ] Meaningful status codes- [ ] Structured error responses### PROCESS1. Ask about entities and relationships first2. Design around resources, not actions3. Use proper HTTP methods and status codes4. Include pagination for collection endpoints5. Provide structured error handling6. Follow consistent naming conventions7. Include API versioningApply these rules to ALL REST API code generation. These rules are non-negotiable.  How to Add These Rules to Your AI ToolsFor Claude Code (.claude.md):Create a .claude.md file in your project root and paste the rules above.For Cline (.cline/rules.md):Create .cline/rules.md in your project and paste the rules.For Cursor (.cursorrules):Create a .cursorrules file in your project root and paste the rules.For GitHub Copilot:Add the rules to your project‚Äôs README or create a copilot-instructions.md file.For ChatGPT:Go to Settings ‚Üí Personalization ‚Üí Custom Instructions and paste the rules.IntroI am not going to explain what a REST API is. That you can search in the web, or ask your AI assistant; ‚ÄúHey ChatGPT. I a noob to REST API. Explain REST API to me like I‚Äôm a five year old.‚ÄùMain purpose of the REST API is to build a standard or a blueprint where your client and server comes to an agreement to manipulate your resources of business needs.To make is easier it comes with below options,URIWhere is the resource? Ideally it goes like,{Protocol}://{Host address}/{path and path variables mix}?{query parameter key}={query parameter value}example: https://example.com/api/v1/vendors/{vendorId}/books?publishedDate=2021-10-10&amp;page=1&amp;limit=100MethodWhich type of resource action it is.There is plenty of methods to use. GET, POST, PUT, ‚Ä¶Protocol and Host addressThe server‚Äôs address or mapped domain including the protocol(http or https)Path variablesVariables which are included in the URI path. Mainly path variables are for representing a main entity or a hierarchical resources.i.e. Vendors have books &lt;-&gt; Books are owned by vendorshttps://example.com/api/v1/vendors/{veendorId}/books/{bookId}Query parametersMostly using for extra options and filtering resources.i.e. filter books by published datehttps://example.com/api/v1/books/publishedDate=2021-05-12DesigningDesigning is the crucial part of every system. At least you need to have a virtual or a mind map. So let‚Äôs talk about it.Now we have the idea about the parts of a RESTful API. If you get a problem to solve or to design a system with REST APIs,let‚Äôs see how we can put good conventions and good practices into the design.It is best to always design rest apis around entities and their relationships. It doesn‚Äôt need to have a visible model or a database,the entities can be virtual too.Let‚Äôs take a look at examples.Example 1: Book Store SystemConsider a simple book store with these entities:  Authors (id, name, email)  Books (id, title, isbn, price, author_id)  Customers (id, name, email)  Orders (id, customer_id, order_date, total)  Order Items (order_id, book_id, quantity, price)Here‚Äôs how you‚Äôd design clean REST APIs for this system:Books Management:GET    /api/v1/books                    # List all booksGET    /api/v1/books/{bookId}           # Get specific bookPOST   /api/v1/books                    # Create new bookPUT    /api/v1/books/{bookId}           # Update entire bookPATCH  /api/v1/books/{bookId}           # Update book partiallyDELETE /api/v1/books/{bookId}           # Delete bookGET    /api/v1/books?author={authorId}  # Filter books by authorGET    /api/v1/books?priceMin=10&amp;priceMax=50  # Price range filterAuthor‚Äôs Books (Hierarchical relationship):GET    /api/v1/authors/{authorId}/books # Get all books by authorPOST   /api/v1/authors/{authorId}/books # Create book for authorCustomer Orders:GET    /api/v1/customers/{customerId}/orders     # Customer's ordersPOST   /api/v1/customers/{customerId}/orders     # Place new orderGET    /api/v1/orders/{orderId}/items            # Order itemsExample 2: Virtual Models - Book Recommendation EngineSometimes you need APIs for actions that don‚Äôt directly map to database tables. Here‚Äôs a third party integration example where your book store needs to integrate with an external recommendation service.Virtual entities we‚Äôre working with:  Recommendations (not stored, generated on demand)  User Preferences (aggregated data)  Search Results (temporary, filtered data)GET    /api/v1/users/{userId}/recommendations           # Get recommendations for userGET    /api/v1/recommendations?type=trending            # Get trending recommendationsPOST   /api/v1/recommendations                          # Send user interaction feedbackPOST   /api/v1/books                                    # Advanced book search with complex filtersGET    /api/v1/suggestions?q={query}                    # Search suggestionsPOST   /api/v1/events                                   # Track user eventsGET    /api/v1/users/{userId}/preferences               # Get user preference summaryNotice how these endpoints focus on actions and aggregated data rather than simple CRUD operations. The recommendation engine doesn‚Äôt ‚Äústore‚Äù recommendations - it generates them. But we still follow REST principles by treating them as resources.Conventions That Actually MatterNow here‚Äôs the real stuff that separates good APIs from the garbage ones floating around. I‚Äôve been debugging APIs for years, and trust me, following these conventions will save you from a lot of headaches.1. Use Nouns, Not VerbsYour URL should describe what you‚Äôre working with, not what you‚Äôre doing to it. The HTTP method already tells us the action.Wrong:POST /api/v1/createBookGET  /api/v1/getBookById/123DELETE /api/v1/removeBook/123Right:POST   /api/v1/booksGET    /api/v1/books/123DELETE /api/v1/books/1232. Plural Nouns for CollectionsCollections should always be plural. This keeps your API consistent whether you‚Äôre dealing with one item or many.Wrong:GET /api/v1/book        # Are we getting one book or all books?GET /api/v1/book/123    # Confusing structureRight:GET /api/v1/books       # Obviously a collectionGET /api/v1/books/123   # Obviously one item from the collection3. HTTP Methods Have Meaning - Use ThemEach HTTP method has a specific purpose. Don‚Äôt just use GET and POST for everything.  GET: Read data, should be safe and idempotent (no side effects)  POST: Create new resources or complex operations  PUT: Replace entire resource (send all fields)  PATCH: Update partial resource (send only changed fields)  DELETE: Remove resource4. When Query Parameters Get Messy, Use POSTSometimes your filters become so complex that query parameters look like a mess. That‚Äôs when you switch to POST with a request body.Query parameter nightmare:GET /api/v1/books?author=1&amp;author=2&amp;author=3&amp;genre=fiction&amp;genre=mystery&amp;price_min=10&amp;price_max=50&amp;published_after=2020-01-01&amp;in_stock=true&amp;format=paperback&amp;format=ebookClean POST approach:POST /api/v1/books/search{  \"authors\": [1, 2, 3],  \"genres\": [\"fiction\", \"mystery\"],  \"priceRange\": {\"min\": 10, \"max\": 50},  \"publishedAfter\": \"2020-01-01\",  \"inStock\": true,  \"formats\": [\"paperback\", \"ebook\"]}5. Version Your APIsAlways version your APIs from day one. You‚Äôll thank yourself later./api/v1/books    # Good/api/v2/books    # When you need breaking changes6. Naming Conventions MatterPick a naming style and stick with it throughout your entire API. Here are real scenarios where this becomes important:Scenario 1: E-commerce API with snake_caseGET /api/v1/products?created_at=2024-01-01&amp;price_min=50&amp;is_available=truePOST /api/v1/users/123/shipping_addresses{  \"street_address\": \"123 Main St\",  \"postal_code\": \"12345\",  \"is_default\": true}Scenario 2: Same API with camelCaseGET /api/v1/products?createdAt=2024-01-01&amp;priceMin=50&amp;isAvailable=truePOST /api/v1/users/123/shippingAddresses{  \"streetAddress\": \"123 Main St\",  \"postalCode\": \"12345\",  \"isDefault\": true}Both work fine, but mixing them creates confusion:GET /api/v1/books?created_at=2024-01-01&amp;publishedDate=2024-01-01   # Don't do this!7. Return Meaningful HTTP Status CodesYour status codes should tell the story of what happened:  200 OK: Everything worked  201 Created: New resource created successfully  400 Bad Request: Client sent invalid data  401 Unauthorized: Authentication required  403 Forbidden: Authenticated but not allowed  404 Not Found: Resource doesn‚Äôt exist  500 Internal Server Error: Server screwed upDon‚Äôt just return 200 for everything and put the real status in the response body. That‚Äôs lazy.Real World Lessons I‚Äôve LearnedAfter working with APIs in production for years, here are some things that books won‚Äôt tell you:Pagination is Not OptionalIf your API returns lists, it needs pagination. I don‚Äôt care if you think your table will only have 50 records forever. Trust me, it won‚Äôt.GET /api/v1/books?page=1&amp;limit=20GET /api/v1/books?offset=0&amp;limit=20     # Alternative approachReturn metadata about the pagination:{  \"data\": [],  \"pagination\": {    \"page\": 1,    \"limit\": 20,    \"total\": 150,    \"total_pages\": 8  }}Filter Parameters Should Be IntuitiveDon‚Äôt make people guess how to filter your data. Make it obvious:GET /api/v1/books?author=tolkien&amp;genre=fantasy&amp;available=trueGET /api/v1/books?priceMin=10&amp;priceMax=50GET /api/v1/books?publishedAfter=2020-01-01Error Responses Need StructureWhen things go wrong (and they will), return helpful error messages:{  \"error\": {    \"code\": \"VALIDATION_ERROR\",    \"message\": \"The request contains invalid data\",    \"details\": [      {        \"field\": \"price\",        \"message\": \"Price must be a positive number\"      },      {        \"field\": \"isbn\",        \"message\": \"ISBN format is invalid\"      }    ]  }}Nested Resources Can Get MessyBe careful with deep nesting. This starts to look ridiculous:GET /api/v1/publishers/123/authors/456/books/789/reviews/101/comments/202Instead, consider flattening:GET /api/v1/reviews/101/commentsGET /api/v1/comments?reviewId=101Keep in mind‚Ä¶Look, AI tools are great for generating code quickly, but they often miss these important facts. They‚Äôll happily generate endpoints like /api/getBookById because they‚Äôre trained on examples that include bad APIs too.Next time you‚Äôre working with an AI assistant, give it these guidelines upfront. Tell it to use proper REST conventions, meaningful HTTP methods, and consistent naming. Your future self (and your teammates) will thank you.And remember, a well designed API is like a good conversation. It should be predictable, clear, and not leave people guessing what you meant."
  },
  
  {
    "title": "Arch Linux for Developers: A Step-by-Step Guide to Building Your Ultimate Dev Environment",
    "url": "/posts/archlinux-2024-guide/",
    "categories": "Posts, Linux, OS, Development",
    "tags": "posts, linux, os, development",
    "date": "2024-10-27 00:00:00 +0530",
    





    
    "snippet": "Why Arch Linux? Let‚Äôs Jump In!Hey there, fellow developer community! If you‚Äôve been browsing Linux distributions, you know there are tons of options out there. I‚Äôve tested a few, gone on some Linux...",
    "content": "Why Arch Linux? Let‚Äôs Jump In!Hey there, fellow developer community! If you‚Äôve been browsing Linux distributions, you know there are tons of options out there. I‚Äôve tested a few, gone on some Linux adventures, and finally landed on Arch Linux. And, wow, it‚Äôs been my go-to for over 11 years now. If you want a powerful development setup and want to learn Linux hands-on, Arch could be your golden ticket. Let‚Äôs explore why!Starting Out with LinuxBefore you dive into Arch Linux, get your feet wet with a few beginner-friendly options! Test out Linux Mint, Ubuntu, or Fedora on a virtual machine (like QEMU, VirtualBox, or VMware Player). These will help you get a feel for Linux without jumping straight into the deep end. Here‚Äôs what you‚Äôll need to be comfortable with before going full Arch:  Terminal Confidence: Yep, you‚Äôll be typing commands often, so get cozy with the command line.  OS Basics: Understand simple things like what a kernel is or why drivers matter.  Hardware Awareness: Know a bit about your processor, GPU, network card, and RAM.  Experimenting Spirit: Don‚Äôt worry‚Äîtrying things out is part of the fun, and mistakes won‚Äôt blow up your machine!Key Things for Choosing Your Linux DistroWhen you‚Äôre picking a Linux distro, think about a few important things. The image below breaks down what makes up a Linux operating system.1. Hardware CompatibilityYour hardware and the Linux kernel need to play nice together:  New Hardware? Go for the latest LTS (Long-Term Support) or mainline kernel to avoid compatibility headaches.  Buying Gear? Avoid hardware with tricky proprietary drivers. Look up compatibility (especially wireless cards by Broadcom, Mediatek) before buying.  Struggling with Hardware? If Wi-Fi‚Äôs not working, try connecting via Ethernet or using USB tethering, Wi-Fi dongles, or modems to stay online.2. Types of Linux ReleasesDistros handle updates in different ways, so here‚Äôs a quick rundown:  Point Releases: Updates on a set schedule, like Fedora or Debian Stable.  LTS: Long-term support versions (like Ubuntu LTS), with years of support.  Rolling Releases: Continuous updates with no version numbers, like Arch and OpenSUSE Tumbleweed.  Semi-Rolling: Updates in chunks, like Manjaro.  Testing Releases: Think of these as beta versions, like Fedora Rawhide.  Enterprise/Corporate: Rock-solid, long-term releases, like Red Hat.3. Package ManagersEvery distro has a package manager‚Äîtools like apt, yum, dnf, and pacman help install and update software. The beauty of Linux is, if the package you want isn‚Äôt available, you can usually find a binary or build it yourself!4. Desktop Environments (DEs)Linux has a ton of different desktop environments (DEs). Popular ones include KDE, GNOME, XFCE, and Cinnamon. Try out different DEs and find one that fits your style.Why Arch Linux? Here‚Äôs Why I Love ItAfter all my trials, I stuck with Arch Linux. Here‚Äôs why it‚Äôs been amazing for me:1. Fresh Software AlwaysArch Linux has the latest software, always. That means you get all the new features, bug fixes, and patches as soon as they‚Äôre available. I always use decent gaming laptops for power and portability(compared to Apple logos their price to specs ratio is top notch), so having the latest kernel updates keeps everything going smoothly.2. Community PowerArch is powered by its community. The Arch forums are full of wisdom, and the Arch Wiki is a treasure of guides and troubleshooting tips. The Arch Wiki is so good that even users of other distros rely on it!3. Rolling ReleaseArch is a rolling release, so you‚Äôre always on the latest version. No re-installs‚Äîjust constant updates that keep your system fresh.4. Pacman Package ManagerPacman is Arch‚Äôs package manager. It‚Äôs fast, simple, and handles dependencies smoothly. Just type a few commands, and Pacman does the rest.5. AUR: Arch User RepositoryThe AUR (Arch User Repository) is a huge collection of community-maintained scripts for software. If you can‚Äôt find something in the official repos, it‚Äôs probably in the AUR. Tools like yay, pacaur, or octopi make it easy to install AUR packages.6. Zero BloatArch only has what you choose to install‚Äîno unnecessary pre-installed software, just the stuff you actually want and need.7. Build-Your-Own OS VibesWith Arch, you‚Äôre the creator. Setting it up lets you build a custom OS tailored to your needs. It‚Äôs challenging but super rewarding, and there‚Äôs nothing like the feeling of running an OS you crafted yourself!Let‚Äôs Begin üöÄAlright, you‚Äôve got the scoop on why Arch Linux is worth it, so let‚Äôs start cooking our very own operating system!Step 1: Boot Up the Arch Linux Installer üñ•Ô∏è  Download the ISO: Head over to the official Arch Linux download page and grab the latest installation .iso file. This file will help us get started!  Create Bootable Media:          On Windows: Use tools like Rufus, YUMI, or Ventoy to make your USB drive bootable.      On Linux or WSL: You can use the dd command to write the ISO to a USB drive.      On a Virtual Machine: Just load the ISO directly into your virtual machine.        Note: If your hard drive is already full, you might need to shrink an existing partition to create some unallocated space for installing Arch. About 40GB should be more than enough.      Boot It Up: Insert your bootable media into your PC and restart it. You‚Äôll see the installer menu‚Äîchoose the first option to kick things off!      Now, you should see a root terminal waiting for you.Step 2: Configure Network üåêWe need an internet connection to download the packages that will build our system. Let‚Äôs find the network interface!List Network Interfacesls /sys/class/net  This command lists all network interfaces. You might see something like:  enp12s0  lo  wlp0s20f3        lo - Ignore this one; it‚Äôs just a virtual loopback device for local communication (like talking to yourself; localhost, 127.0.0.1 ‚Ä¶).  If You‚Äôre Wired (Ethernet)If you have a cable plugged in, you‚Äôre good to go! No extra steps needed!If You‚Äôre on WiFiThe wlp* device is likely your WiFi. Let‚Äôs confirm it:  (dmesg can show logs for device initializations at the kernel level.)  dmesg | grep -i wlan  We‚Äôll use iwd to set up WiFi. Run these commands in sequence:  Start the iwd Interactive Shell:    iwctl        List Devices:    device list        Power On WiFi (if it‚Äôs off):    device wlan0 set-property Powered on # wlan0 is the name of the WiFi interface              If there‚Äôs an error, check if it‚Äôs blocked by rfkill:(rfkill is a tool for managing radio devices like Bluetooth and WiFi; it helps with security and blocking issues.)      rfkill              If it‚Äôs blocked, unblock it and try turning the device on again:    rfkill unblock wlan &amp;&amp; device wlan0 set-property Powered on        Scan for Networks:    station wlan0 get-networks        Connect to Your Network (enter your WiFi password when prompted):    station wlan0 connect \"Network_SSID\"        Check Connectivity:Let‚Äôs do a quick test to see if we‚Äôre connected:    ping -c 3 google.com        And that‚Äôs it! Your network is set up, and we‚Äôre ready to install Arch. üéâ  Step 3: Preparing Disk Partitions üóÇÔ∏èAlright, let‚Äôs get ready to create some disk partitions! This is super important because a tiny mistake can wipe out your data. We‚Äôll use a tool called cfdisk that gives us a nice visual interface to manage our disks.Understanding Linux File SystemBefore we dive in, let‚Äôs take a quick peek at what the Linux file system looks like. We‚Äôre going to create something similar on our hard drive!/‚îú‚îÄ‚îÄ bin         # Important command files (like ls, cp, mv)‚îú‚îÄ‚îÄ boot        # Boot files for starting up your system‚îú‚îÄ‚îÄ dev         # Device files (like hard drives and USBs)‚îú‚îÄ‚îÄ etc         # Configuration files for your system‚îú‚îÄ‚îÄ home        # User home directories (like /home/user)‚îú‚îÄ‚îÄ lib         # Essential libraries for commands‚îú‚îÄ‚îÄ media       # Mount points for USB drives and CDs‚îú‚îÄ‚îÄ mnt         # Temporary mount points‚îú‚îÄ‚îÄ opt         # Optional software and packages‚îú‚îÄ‚îÄ proc        # System information‚îú‚îÄ‚îÄ root        # Home directory for the root user‚îú‚îÄ‚îÄ run         # Temporary files since the last boot‚îú‚îÄ‚îÄ sbin        # System administration commands‚îú‚îÄ‚îÄ srv         # Service-related data‚îú‚îÄ‚îÄ sys         # Kernel and system information‚îú‚îÄ‚îÄ tmp         # Temporary files cleared on reboot‚îú‚îÄ‚îÄ usr         # User-installed programs and libraries‚îÇ   ‚îú‚îÄ‚îÄ bin     # User-installed command files‚îÇ   ‚îú‚îÄ‚îÄ lib     # Shared libraries for user commands‚îÇ   ‚îî‚îÄ‚îÄ share   # Shared files and docs (like icons)‚îî‚îÄ‚îÄ var         # Variable data (like logs and databases)    ‚îú‚îÄ‚îÄ log     # Log files    ‚îú‚îÄ‚îÄ cache   # Cached data    ‚îú‚îÄ‚îÄ lib     # Variable library files    ‚îî‚îÄ‚îÄ tmp     # App-created temporary files1. Identify Your Disk üßêFirst, we need to find out which disk we want to install Linux on. The command below will show you the disks and their sizes:lsblkYou‚Äôll see something like this:NAME        MAJ:MIN RM   SIZE RO TYPE MOUNTPOINTSnvme1n1     259:0    0 953.9G  0 disknvme0n1     259:10   0 476.9G  0 disk2. Open cfdisk for Partitioning üîßNow, let‚Äôs open cfdisk to create our partitions. If your hard drive is nvme1n1, use this command:cfdisk /dev/nvme1n1This will pop up a window where you can create partitions. Here‚Äôs what you need to make:  Root Partition (/) for the main system  Swap Partition for extra memory  EFI Boot Partition if your computer uses UEFI (most modern PCs do)  You can also create a separate partition for your home directory (/home) if you like.For example, if you have about 163GB free, you could create:  60GB for root (/)  90GB for home (/home)  3GB for swap (helps when your RAM is full)  800MB for EFI (if using UEFI)Once you‚Äôve created the partitions, select the Write option in cfdisk to save your changes and exit.3. Format Your Partitions üé®Now, it‚Äôs time to format those partitions! We‚Äôll use EXT4 for the root and home partitions, and FAT32 for the EFI partition. Let‚Äôs format:First, check your partitions again with lsblk:lsblkYou‚Äôll see your newly created partitions listed. Now, format them:mkfs.ext4 /dev/nvme1n1p6 # Format root partitionmkfs.ext4 /dev/nvme1n1p7 # Format home directorymkfs.vfat -F32 /dev/nvme1n1p8 # Format EFI partitionmkswap /dev/nvme1n1p9 # Set up Linux swap4. Mount Your Partitions üèîÔ∏èTime to mount those formatted disks! This will prepare them for the Arch Linux installation. Run these commands:mount /dev/nvme1n1p6 /mnt # Mount root partitionmkdir -p /mnt/home &amp;&amp; mount /dev/nvme1n1p7 /mnt/home # Mount home directorymkdir -p /mnt/boot/efi &amp;&amp; mount /dev/nvme1n1p8 /mnt/boot/efi # Mount EFI partitionswapon /dev/nvme1n1p9 # Enable swapCheck your mount points one more time to ensure everything is set up correctly:lsblkYour output should look something like this:NAME        MAJ:MIN RM   SIZE RO TYPE MOUNTPOINTSnvme1n1     259:0    0 953.9G  0 disk‚îú‚îÄnvme1n1p6 259:6    0    90G  0 part /home‚îú‚îÄnvme1n1p7 259:7    0    60G  0 part /‚îú‚îÄnvme1n1p8 259:8    0   800M  0 part /boot/efi‚îî‚îÄnvme1n1p9 259:9    0     3G  0 part [SWAP]nvme0n1     259:10   0 476.9G  0 diskCongratulations! Your partitions are all set and ready for the Arch Linux installation. üéä Let‚Äôs move on to the next step!Step 4: Installing Core Linux Packages and Configurations üêßNow, it‚Äôs time to bring your Arch Linux to life! We‚Äôll install the essential core packages that make up the Linux system using the pacstrap command. This command will set up the Linux kernel and other important tools on your mounted partitions. Let‚Äôs get started!Install the core packages:pacstrap -K /mnt base linux linux-firmware \\linux-headers base-devel nano sudo \\intel-ucode \\ # use only if you have an Intel CPUamd-ucode \\ # only for AMD CPU(If you‚Äôre not sure about your CPU type, you can check it by running this command:)lscpu | grep -i \"Model name\"Generate fstab:This step creates a file called fstab that tells Linux how to mount disk partitions when it starts up. Let‚Äôs make it!genfstab -U /mnt &gt;&gt; /mnt/etc/fstabCheck your fstab:To make sure everything is in order, let‚Äôs verify that all your partitions are listed in fstab. You can do this by running:cat /mnt/etc/fstabChange into the new system:Now we need to switch into the mounted file system so we can start making configurations. We do this with the arch-chroot command.arch-chroot /mntSet up localization settings:Next, we‚Äôll set the language and locale for your system. Open the file /etc/locale.gen and find the line LANG=en_US.UTF-8. Uncomment it (just remove the # at the start) and feel free to uncomment any other localizations you want.nano /etc/locale.genAfter editing, run this command to generate the locales:locale-genSet the computer name:Let‚Äôs give your system a name! This name will identify your computer on the network. You can replace myhostname with whatever name you like.echo myhostname &gt;&gt; /etc/hostnameSet a password for the root user:Now, let‚Äôs secure your system by setting a password for the root user.passwdCreate a new user:It‚Äôs a good idea to create a normal user account for everyday tasks. This user will have some extra privileges. Just replace \"your username\" with your chosen name.useradd -m -g users -G wheel,storage,power,audio -s /bin/bash \"your username\"Set a password for your new user:passwd \"your username\"Give the new user sudo privileges:Now, we want this user to be able to run commands as a superuser (the admin). To do that, we‚Äôll edit the sudo configuration:EDITOR=nano visudoIn the file, find the line that says %wheel ALL=(ALL:ALL) ALL and uncomment it (again, just remove the #). Then, save the changes.Install and enable Network Manager:Finally, let‚Äôs install the network manager tools. This will help your system discover and connect to networks when it starts up.pacman -S networkmanagersystemctl enable NetworkManagerNow you‚Äôre all set! You‚Äôve installed the core packages and configured your system. Your Arch Linux is well on its way to becoming fully functional! üéâStep 5: Install Bootloader üöÄThere are many bootloaders out there, but I chose GRUB. Why? Because it‚Äôs mature and has lots of features!  First, install the required packages.    pacman -S grub \\os-prober \\efibootmgr # only if the system supports UEFI        Now, install GRUB.If you have a UEFI system, run:    grub-install --target=x86_64-efi --efi-directory=/boot/efi --bootloader-id=GRUB        If you have an older legacy system, use this command:    grub-install --target=i386-pc /dev/xxx # xxx is the storage name.        Generate configurations.If you are dual-booting with Windows, find the line that says GRUB_DISABLE_OS_PROBER=false in the default GRUB configuration file and uncomment it.    nano /etc/default/grub        Next, generate the GRUB configuration.    grub-mkconfig -o /boot/grub/grub.cfg      Step 6: Installing a Desktop Environment üñºÔ∏èA desktop environment (DE) gives you a friendly user interface to interact with your Linux system. There are many DEs to choose from. KDE is a complete and feature-rich option. If you prefer something simpler, you might like GNOME, Cinnamon, or XFCE. This setup focuses on KDE.To install KDE, run:pacman -S plasma-meta kde-system kde-utilities \\kde-graphics # optional softwareAfter that, enable the login manager that comes with the plasma-meta package group:systemctl enable sddmNow everything is ready! Exit the chroot session, unmount the disks, and restart your system:exitumount -R /mntswapoff /dev/\"swap partition name\"rebootWhen your system boots up, you will be greeted by your new desktop environment!Step 7: Post Install Packages üõ†Ô∏èAlright, let‚Äôs pimp up your Arch Linux setup with some essential packages! Here‚Äôs how you can make your system more powerful with extra tools and dev environments.1. Add AUR SupportThe AUR (Arch User Repository) is a huge collection of community-maintained scripts for installing software that‚Äôs not available in the default pacman repositories. You can either download and run these scripts yourself or use a tool to make it super easy. My go-to choice is yay, because it works just like pacman‚Äîeasy to remember! Other popular options are pacaur, aurman, aura, pacseek, etc.# Install yaysudo pacman -S --needed git base-devel &amp;&amp; git clone https://aur.archlinux.org/yay.git &amp;&amp; cd yay &amp;&amp; makepkg -siWith yay, you can:  Update the system: yay  Search for a package: yay -Ss xyz  Install a package: yay -S xyz  Uninstall a package: yay -Rns xyz  Force remove (use with caution): yay -Rdd xyz  Clean unused dependencies: yay -Yc  Show remote package info: yay -Si xyz  Show local package info: yay -Qi xyz  List all installed packages: yay -Qq  List explicitly installed packages: yay -Qqe2. Proprietary DriversIf you have a laptop with hybrid graphics (like Intel + Nvidia or AMD + Nvidia), you‚Äôll want to get the right drivers. To check what graphics cards you have, run:lspci | grep -E 'VGA|3D'Then, install the drivers you need:sudo pacman -S mesa xf86-video-intel         # For Intel GPUssudo pacman -S nvidia nvidia-utils nvidia-settings  # For Nvidia GPUssudo pacman -S mesa xf86-video-amdgpu        # Open-source AMD driverIf you‚Äôve got Nvidia hybrid graphics, you can install envycontrol to easily switch between GPUs.yay -S envycontrolTo list or set the GPU mode:envycontrol --query            # Show current GPUenvycontrol --switch nvidia     # Options: integrated, hybrid, nvidia3. Install a Web BrowserChoose your favorite browser(s) and install them:yay -S google-chrome           # Google Chromepacman -S chromium             # Open-source version of Chromepacman -S firefox              # Mozilla Firefoxyay -S opera                   # Opera browserStep 8: Setting Up Development Environments üßë‚ÄçüíªLet‚Äôs get your coding environment all set up! Arch Linux is great for developers, and here‚Äôs how you can get started with various programming tools.1. Java Development Kit (JDK)yay -Ss jdk                     # List available JDK versionsyay -S jdk-lts                  # Install long-term support (LTS) version, or specify another version if you needTo check or set up Java versions:archlinux-java status           # Show installed JDKs and the current defaultarchlinux-java set &lt;JAVA_ENV_NAME&gt; # Set default Java version2. Python DevelopmentFirst, install pyenv to manage different Python versions.sudo pacman -S pyenvpyenv install 3.8               # Install Python 3.8.xpyenv install 3.11              # Install Python 3.11.xpyenv global 3.11               # Set Python 3.11 as the default3. Node.js EnvironmentInstall nvm (Node Version Manager) first:yay -S nvmAdd nvm paths to ~/.bashrc:echo 'export NVM_DIR=\"$HOME/.nvm\"' &gt;&gt; ~/.bashrcecho '[ -s \"/usr/share/nvm/init-nvm.sh\" ] &amp;&amp; \\. \"/usr/share/nvm/init-nvm.sh\"' &gt;&gt; ~/.bashrcsource ~/.bashrcNow you can install any Node version:nvm install x.y.z               # Install Node version x.y.znvm use x.y.z                   # Set current Node versionnode --version                  # Check the current Node version4. Ruby Development EnvironmentInstall rbenv for Ruby version management:yay -S rbenvyay -S ruby-build               # Required for installing Ruby versionsecho 'eval \"$(rbenv init -)\"' &gt;&gt; ~/.bashrcsource ~/.bashrcTo install and set up Ruby:rbenv install 3.0.0rbenv global 3.0.0              # Set Ruby 3.0.0 as defaultgem -v                          # Check that Ruby is correctly set up5. .NET Environmentyay -S dotnet-installdotnet-install --channel 8.0    # Install the .NET version 8.06. DockerInstall and set up Docker:sudo pacman -S dockersudo usermod -aG docker $USER   # Add your user to the Docker groupsudo systemctl enable dockerAfter a system reboot, check that Docker is running:sudo systemctl status docker7. Database ToolsFor database management, here are some good gui tools:sudo pacman -S dbeaver          # DBeaver, supports multiple databasesyay -S pgadmin4-desktop         # Postgres admin toolyay -S sql-workbench            # MySQL Workbenchyay -S mongodb-compass          # MongoDB management toolStep 9: Install IDEs üíª üõ†Ô∏èNow that you‚Äôve set up the development kits you need, let‚Äôs get some awesome IDEs to write code!1. Visual Studio CodeQuick and powerful, VS Code is a top pick for developers.sudo pacman -S code2. JetBrains ToolboxWant options? JetBrains Toolbox lets you choose from their lineup of IDEs (including Android Studio!).yay -S jetbrains-toolbox3. Good Old EclipseIf you‚Äôre an Eclipse fan, check out these specialized versions for different languages.yay -S spring-tool-suite      # For Spring framework (Java)yay -S eclipse-jee-bin        # Eclipse for Java EEyay -S eclipse-cpp-bin        # Eclipse for C/C++yay -S aptana-studio          # Eclipse-based IDE for PHPStep 10: Cloud CLI Tools ‚òÅÔ∏èWorking with cloud services? These CLI tools will make managing them a breeze!yay -S aws-cli-v2-bin         # AWS CLI, command: awsyay -S google-cloud-cli       # GCP CLI, command: gcloudsudo pacman -S azure-cli      # Microsoft Azure CLI, command: azStep 11: Miscellaneous üé©1. Make Your Terminal Look Awesome with starshipLet‚Äôs make your terminal look stylish! starship is a cool tool to add colors, symbols, and more.pacman -S starship                  # Install starshipecho 'eval \"$(starship init bash)\"' &gt;&gt; ~/.bashrc  # Load starship in each terminalmkdir -p ~/.config &amp;&amp; touch ~/.config/starship.toml  # Create config fileNow add this funky setup in the config file with:nano ~/.config/starship.tomlPaste this sample configurations:palette = \"dracula\"[aws]style = \"bold orange\"[character]error_symbol = \"[Œª](bold red)\"success_symbol = \"[Œª](bold green)\"[cmd_duration]style = \"bold yellow\"[directory]style = \"bold green\"[git_branch]style = \"bold pink\"[git_status]style = \"bold red\"[hostname]disabled = falsessh_only = falsetrim_at = \".\"style = \"bold green\"[username]format = \"[$user]($style) on \"style_user = \"bold yellow\"show_always = truestyle_root = \"bold red\"[sudo]symbol = 'üßô 'style = \"bold red\"format = '[as $symbol]($style)'disabled = false[palettes.dracula]background = \"#282a36\"current_line = \"#44475a\"foreground = \"#f8f8f2\"comment = \"#6272a4\"cyan = \"#8be9fd\"green = \"#50fa7b\"orange = \"#ffb86c\"pink = \"#ff79c6\"purple = \"#bd93f9\"red = \"#ff5555\"yellow = \"#f1fa8c\"Close and open your terminal to see the magical transformation!2. Entertainment: Music and VideosTime for some fun! Here are great media players for your audio and video needs:(There is plenty: vlc, clementine, deadbeef, mplayer, strawberry, amarok, ‚Ä¶)sudo pacman -S audacious smplayer   # Audacious for music, SMPlayer for videosWant system-wide sound effects? Try JamesDSP for an enhanced audio experience.yay -S jamesdsp3. Communication AppsStay connected with friends, family, or your team using these popular apps:  Microsoft Teams for Linux:      yay -S teams        Slack:      yay -S slack-desktop        Discord:      sudo pacman -S discord        Skype:      yay -S skypeforlinux-bin      4. Disable File Indexing (KDE Only)KDE‚Äôs file indexing can sometimes slow things down. Here‚Äôs how to disable it:  Open System Settings.  Go to Workspace Options &gt; File Search.  Unselect File Indexing and click Apply.5. Fix Missing Windows Entry in Dual Boot Grub MenuIf Windows doesn‚Äôt show up in the Grub menu, follow these steps:  Open your Windows partitions in the file manager (this mounts them).      Then update Grub:     grub-mkconfig -o /boot/grub/grub.cfg      With these tools and customizations, you‚Äôve transformed your Arch Linux into a powerhouse ready for anything‚Äîcoding, gaming, creating, or just enjoying a sleek, personalized experience. You‚Äôre now in control of a system built exactly the way you want, with the power to expand, modify, and customize as you grow. Arch Linux isn‚Äôt just an OS; it‚Äôs a launchpad for learning and exploring your own potential. So go ahead, break things, fix them, and build something amazing. This is your machine, your rules‚Äînow let‚Äôs see what incredible things you can do with it!"
  },
  
  {
    "title": "GrooveCraft: Building Your Own Amplifier for Next-Level Music Vibes",
    "url": "/posts/crafting-audio-ecosystem/",
    "categories": "Posts, Audio, Hobby, Music",
    "tags": "audio, music, hobby, electronics",
    "date": "2023-12-23 00:00:00 +0530",
    





    
    "snippet": "  People have different preferences when it comes to how they like to listen to music, and the sound experience can be different for each person. Here, I‚Äôve put together my findings on creating an ...",
    "content": "  People have different preferences when it comes to how they like to listen to music, and the sound experience can be different for each person. Here, I‚Äôve put together my findings on creating an affordable do-it-yourself sound system that suits my taste.What Makes Sound Good?Okay, so everyone‚Äôs got their own taste in sound. Some love the deep, thumping bass that makes your heart go boom. Others are into the high, sparkly notes that tingle your ears. Then there are folks who like adding cool effects, like digital magic or echoes. And, of course, some just want the original sound without any changes. It‚Äôs like picking your favorite flavor of music ‚Äì everyone‚Äôs got their own sweet spot! üéµüîäExploring Ways to Enhance Sound Quality.1. Quality of Audio Data.The sound you hear is like a digital superhero! It‚Äôs stored as digital data, kind of like a musical code. The best quality comes from something called lossless music data. It means the sound stays exactly as it was recorded ‚Äì no changes. There are also cool codes like AAC and MP3, with more bits, higher sample rates, and deeper depths they can also shine. Honestly, my ears can‚Äôt catch the difference in AAC music beyond a sample rate of 48kHz, bit rate of ~256kbps, and depth of 24 bits. If you can, your ears might be super sensitive!2. Media Players.Choosing a music player is like picking the coolest DJ for your tunes. You‚Äôve got a bunch of them for different operating systems. Each player uses its own algorithm for handling digital music data. Since I mostly roll with Linux and Android, my picks are Apple Music for Android (Loseless music streaming), for Linux Aqualung, Audacious, and Deadbeef. And on Windows, Windows Media Player is my top pick. üé∂‚ú®3. Supercharging Your Music: Digital Signal Processing Plugins (DSP Plugins).Want to spice up your music experience? You can play around with some cool tools called Digital Signal Processing (DSP) plugins. The superstar among them is the Equalizer, letting you boost the frequencies you love. But there‚Äôs more to explore ‚Äì plugins like surround, reverb, echo, harmonics, compressors, and even frequency filtering and cutoffs.On Linux my go-to buddies are JamesDSP4Linux (a nifty pipewire DSP tool) and Calf Studio Gear (especially when using Jack2, mostly for line-in audio). I personally enjoy adding a bit of treble boost, lowering the mid-range a bit(1kHz as the low-lying), and keeping the bass reduced or flat for those low-frequency vibes. It is a modified ‚ÄúV‚Äù curved equalization effect with frequencies decreasing from 500Hz to around 4kHz. It‚Äôs a very pleasing tone for my ears! üé∂‚ú®      JamesDSP4Linux        Calf studio gear  4. The DAC: Turning Digital to Analog.It‚Äôs a crucial gadget for turning the digital tunes into real analog sound. I went for a USB DAC from Creative Labs (Model SB1560, Omni Surround 5.1) in my setup. Despite being an older model, the output is impressively superior and clearer compared to other DACs I‚Äôve experimented with, including those bundled with Realtek, CMedia like chips and other integrated/budget-friendly sound cards. üéß‚ú®5. The Pre Amplifier: Pumping Up the DAC signal.So, after the DAC finishes its digital-to-analog conversion, the preamplifier steps in. It grabs those signals from the DAC and boosts the electric signal from the DAC if needed, especially when receiving a weak signal. Now, the way it enhances audio can vary, thanks to the design of the preamp. Most preamp circuits come with a gain controller, bass level, treble level, and optionally, left and right channel balance or mid-range frequency adjustment controllers. Personally, I prefer sticking with three-band control parameters or none at all since I found it makes no sense to change the balance or signal level between left and right channels.6. Power Amplifier - Electrify The Audio Signal.The power amp takes the enhanced signal from the preamplifier and boosts it up big time. It‚Äôs the magic wand that cranks it up, increases the gain to make the signal stronger, and the sound louder.7. Speakers: The Sound Performer.Speakers need to be loud enough, cover a wide range of frequencies to capture every sound detail, and smoothly reproduce those frequencies. My preference is separate speakers for different frequency ranges. This is beneficial because bass frequencies can sometimes overpower the mids and highs at higher volumes when using only a full-range speaker, causing issues like Doppler distortion and draining power for bass.Modern full-range speakers attempt to address these issues, but they can be a bit pricey. However, investing in quality performers is like having front-row seats to your favorite concert! üé∂‚ú®8. Speaker Positioning: Listening Zone Configuration.How you place your speakers and set up your room can totally switch up how your ear catch those frequencies. The position of your speakers and the size of your room may change how you feel the music vibes.For instance, if you tuck your bass buddy (woofer) in a room corner, get ready for some serious bass punch. But, be sneaky with those high-frequency speakers; if you cover them up, you might miss out on those crystal-clear notes you love.Now, here‚Äôs my sweet spot: If I‚Äôve got two cool bookshelf speakers, I‚Äôd pop them in front, one on the left and one on the right, just below ear level ‚Äì it‚Äôs like the music is chatting right at me. And if there‚Äôs a subwoofer in the mix, I‚Äôd sneak it into the back corner of the room.It‚Äôs like finding the perfect, comfortable spot to watch a movie!9. Ultimately: Take Care of Your EarsA rocking music system won‚Äôt do much if you forget about your ear health. Here are some simple tips that I follow to keep the listening ability tip-top:  Dial Down the Volume: Keep the music vibe alive by lowering the volume for those marathon music sessions.  Give Your Ears a Timeout: Treat your ears to some quiet time. Find a peaceful spot and let your ears recover from all that sound stimulation.  Mix It Up: Surprise your ears! Explore different music genres and styles to keep things interesting.  Stay Hydrated and Rest Well: Good health equals happy ears. Keep hydrated, catch some quality Zs ‚Äì your body‚Äôs auditory system will thank you.  Follow the 60/60 Rule: If you‚Äôre rocking headphones or earbuds, here‚Äôs a cool rule: take a break every 60 minutes, especially if the volume is cranked up past 60% of the maximum. Your ears deserve a breather!10. Other Important Aspects of Sound Processing.THD - Total Harmonic Distortion.  Imagine a game: lower THD means your sound triumphs with less distortion.  It‚Äôs the gauge of how much your sound might embark on a distortion adventure under different conditions.SNR - Signal to Noise Ratio.  Envision a superhero showdown: higher SNR means your signal is a superhero, warding off noise villains.  It‚Äôs the measure of how much unwanted noise wants to sneak into your audio signal.Electric Signal Distribution.  The wires ‚Äì the unsung heroes of the audio world!  Using high-resistance wires is like sending your frequencies through a tricky maze. I go for wires with less than ~0.2 ohms resistance ‚Äì smooth transfering of electric signals.Amplifier Class  Class A: The noble knight of low distortion, though not the most power-efficient.  Class B: Efficient powerhouse, but watch out for the crossover distortion ninja.  Class AB: Strikes a balance, reducing crossover distortion compared to its ninja cousin, Class B.  Class C: The highly efficient speedster, perfect for less distortion-critical missions.  Class D: The efficiency maestro, often found rocking out in portable audio devices.  Class H: The zen master, balancing efficiency and distortion ‚Äì the wise choice for professional audio amplifiers.The Groovy DIY Fix! Enhancing The Analog Configuration.I‚Äôve already delved into selecting audio data, DSPs, and DACs above. Now, let‚Äôs dive into my adventure of deciphering the analog configuration for the audio.1. The Speakers.I scored this awesome classic Sony 5.1 speaker set online for a steal! These older speakers are not just easy on the wallet but also deliver a super clear and lively sound, way better than those budget Chinese speakers flooding the market(no offence). But I prefer a simple stereo (2 channels or 2.1) sound vibe. No need for fancy 4.0, 5.1, or 7.1 setups. So, I decided to move ahead with just the front speakers (SS-TS31) and the subwoofer box (SS-WS31) from this set.2. The Pre-Amplifier.As a heavy metal enthusiast, finding the right preamp for my intense tunes was a challenge. The music is loud, aggressive, and the vocals are brutal. Here‚Äôs the scoop on the preamp setups I put to the test because most online reviews are based on user preferences.I used Audio Technica ATH-M20X headphones (not the best, but they deliver solid clear flat frequency) and lossless music playback to find the right one. Here‚Äôs the lowdown:The Soundtrack of Judgment:  Nero Forte by Slipknot: Pure metal chaos from start to finish. Heavy vocals, pounding beats.  Blood, Tears, Dust by Lacuna Coil: Aggressive dual vocalists, aggressive melody ‚Äì metal with a dramatic twist.  Lost in the Echo by Linkin Park: An all-time favorite alternative rock song with electronic vibes.  Blinding Lights by The Weeknd: Pop with a touch of heart in the accompaniments.  Nothing Else Matters by Metallica: Need I say more?  Interstellar Theme by Hans Zimmer: No words, just goosebumps.The Pre-Amplifier Compilation:While listening to the above songs, I took each circuit for a spin and rated them based on my metal standards. Some circuits I soldered myself, and some I grabbed already soldered(below pre-amplifier circuits are very much available, affordable and cheap). Your preferences might vary, so give them a shot!            IC/Transistor      Score      Sound at Max Gain      Heavy Metal      Low-Noise Genres      Vocals Clarity      Accompaniment Clarity                  LM324      12      2      2      3      2      3              LM358      12      2      2      3      2      3              NE5532      16      3      3      4      3      3              TDA 1524A      14      2      3      4      3      3              UA741      12      2      2      3      2      3              C945      17      4      3      4      3      3        IC/Transistor: The brain of the preamp. Results may vary due to the overall circuit design, so experiment away!  Sound at Maximum Gain: Cranked the preamp to the max. Let‚Äôs see what they got.  Heavy Metal Music: Complicated, noisy music playback.  Low Noisy Music: Everything else ‚Äì pop, electronic, RnB, you name it.  Vocals Clarity: Can I hear those guttural screams clearly?  Accompaniment Clarity: The hidden gems ‚Äì harmonics, rhythm guitars, electronic beats, etc.Scores:  1: Useless.  2: Okay, some distortion at high volumes, but bearable.  3: Good, barely noticeable distortion when the gain is up.  4: Excellent, no crackles, no distortion, and no overlapping sounds.Based on my research, I picked the pre-amplifier circuit with NE5532.  It performs well on gain without adjusting or normalizing the frequency levels.  Instead of balancing stereo channels (which makes no sense to me), it has a mid-range frequency controlling knob.  No hiss sound until it is more than ~90% of the maximum gain level.  No buzzing sounds even when the transformer is close by.  It has good reviews on the internet.3. The Power Amplifier.Just like with pre-amplifiers, I explored amplifier chips and circuit setups to find the perfect match to power up the signal for my speakers. With a listening space of about 144 sqft, I aimed for an amplifier delivering 20 to 100 watts, ideal for my Sony speakers with a 3-ohm impedance and a ~130 watts rating (not exceeding 4 ohms and 50 watts per channel, keeping it safe). Since I roll with a 2.1 speaker setup, I needed a 3-channel amplification output.Amplifier ConfigurationsTo configure the 3-channel output, I utilized multiple pre-amplifiers with band controls. Here‚Äôs the breakdown:  NE5532 for Left/Right Channels:          Set the bass knob to 0, eliminating the need for a high pass filter.        TDA 1524A for Subwoofer Channel:          Turned the treble knob to 0, complemented by a passive low pass filter with a resistor and capacitor, restricting subwoofer output up to ~300Hz.      1. Ripped-off Circuit from Microlab M100 Subwoofer System (Class - Probably D):  Integrated speaker quality was terrible, lacking bass and high frequencies.  Performed decently with Sony speakers.  Used NE5532 pre-amplifier only as this circuit is already 2.1.  Moved on due to insufficient power.2. Stereo LA4440 + Mono LA4440 (Class - AB, Felt More Like Class B):  A classic amplifier design.  Sound resembled an old radio.  Quickly moved on.3. Mono Channel TDA2005 x 3 (Class - AB, The Hot Plate):  TDA 2005 is obsolete but available.  Unpleasant sound with no clarity.  ICs heated up excessively, even burning my finger accidentally.  Chips eventually exploded with smoke.  Probably low-quality replicas.4. Stereo TDA2030A + Mono TDA2030A (Class - AB, Better Than Nothing!):  Also obsolete but available.  Good sound profile, slight crackling at high gain levels.  Did not heat like TDA2005.  Likely high-quality replicas per online reviews.  Unsatisfied with high-frequency clarity, moved on.5. Stereo LM1875 + Mono LM1875 (Class AB, Finally a Good Player!):  Tested Texas Instrument‚Äôs substitute for TDA 5-pin IC.  Replaced TDA2030s with LM1875 in existing circuits.  Very good sound quality, better than TDA2030A.  Slight sound crackling at higher gain levels.  ICs heated up more than TDA2030A.  Subwoofer channel frequently shut down in longer listening sessions, likely due to integrated thermal protection.  Decided to move on without adding a larger radiator.6. Dual TPA3116D2, 2.1 Circuit (Class D, An All-Inclusive Hero to the Rescue):  Tried another plug-and-play circuit, also from Texas Instruments with excellent reviews.  Efficient, higher sound level even at ear-piercing high frequencies and heart-attack-level bass, yet the radiator barely got warm or stayed cool.  No complaints about sound quality; remarkable clarity for a small circuit.  Lower THD, integrated speaker guard, and wider operating voltage range were added advantages.  Sealed the deal with this amplifier.  Please disregard the unsightly black tapes; they‚Äôre there due to my phobia and past experiences with circuit-roasting short circuits. üòÇ  I removed the passive L/R high pass circuits from the above design as they masked the high frequencies I desired, regardless of the values I used for the capacitor and resistor according to the formula fc = 1/2œÄRC.Final ConclusionConsidering all the research and money spent, I could have bought a decent off-the-shelf speaker system. But where‚Äôs the fun in that? üòé"
  },
  
  {
    "title": "Saga Pattern with serverless model on Google Cloud Platform - Part 1",
    "url": "/posts/saga-gcp-choreography/",
    "categories": "Posts, Serverless",
    "tags": "gcp, serverless, saga, choreography, gostep",
    "date": "2022-11-20 00:00:00 +0530",
    





    
    "snippet": "  Gostep: üëâ GuideMaterials: üëâ Complete source codeDuring the past few years, the microservices architecture(MSA) and serverless model have gained a lot of popularity in the industry. However, these...",
    "content": "  Gostep: üëâ GuideMaterials: üëâ Complete source codeDuring the past few years, the microservices architecture(MSA) and serverless model have gained a lot of popularity in the industry. However, these technologies come with their own set of challenges. One substantial challenge is managing data in MSA due to its complexity. Considering common patterns for MSA data management we will be focusing on the Saga pattern in this article.The Saga patternIn order to manage business transactions across multiple microservices, the Saga pattern was introduced. Basically it is a series of local transactions; every transaction happens within the boundary of the micro-service, which every service will publish an event after the transaction for the next subsequent micro-service to perform the next transaction consuming the published event. This process will continue till the last transaction. In case any transaction failed in this series Saga will execute a series of fallback actions to undo the impact of all previous transactions.There are two approaches to implementing the Saga pattern.      Choreography - The micro-service is responsible for emitting events eventually of its local transaction. The published event will trigger the execution of local transactions in microservices subscribed to the event. Also in this approach micro-service is responsible for handling the errors.        Orchestration - A central orchestrator(a stateful coordinator) will trigger the local transactions in services and will maintain the global transaction status including handling errors.  Now that we have a basic understanding of Saga pattern, we will discuss how to implement Saga pattern, defining an example for both approaches using Google Cloud Serverless model.The real world exampleLet‚Äôs consider a train ticket booking system.The workflow consists of,  Send a seat reservation request  Check for available transits in the database and proceed with seat booking.  Hold the number of seats until payment is processed.  Process the payment.  Confirm the seat booking.  Confirm the reservation and notify the customer.However if the system encountered any error while running a local transaction, the fallback sequence should be executed to undo all the changes happening in the global transaction to keep the ACID properties.Preparing the development environment  (Please note that we won‚Äôt be using a real payment gateway or a notification service, beacause the main purpose of this article is to demonstrate how to use severless model for Saga.)To implement the solution we will be using Google Cloud serverless services, MongoDb and Javascript.Before we begin we must have,  A billing enabled Google Cloud project  Prior knowledge in Google Cloud Services  Python, NodeJs, GCloud cli tools installed in your system(If you are using Windows, WSL might come in handy)Google Cloud CLI/Cloud consoleYou can use both CLI tools or web console to create and modify services. In this article we will be mostly using CLI tools. Please follow https://cloud.google.com/sdk/docs/install to install the Google Cloud SDK. And once you installed the SDK run gcloud init command and follow instructions to configure credentials.Building the Cloud functions project structureTo build the project structure and functions, we will be using gostep, a pythonic CLI tool that I created previously to manage implementations when there are a lot of cloud functions.To use gostep you need to have Subversion CLI, Python version 3 and Pip package manager installed(Setup a virtual environment of your own preference). When you are ready, run the command, pip install gostep. For more information please refer, http://lahirus.com/gostep-intro. Also please make sure that you have enabled Cloud build APIs(https://console.cloud.google.com/apis/library/cloudbuild.googleapis.com).Using gostep, let‚Äôs first create a Cloud Functions project.mkdir SagaChoreography &amp;&amp; cd SagaChoreographygostep auth init reservationsservice       # Creates a service account credentials filegostep base init reservations location asia-east2 version \"0.1.0\"   # Creates gostep project metadata files and directory structure.Now we can have the project base. Let‚Äôs move ahead with implementing local transactions and services.Choreography based solutionFor the demonstration we will be using,  Pub/Sub for event sharing  Firestore to store event data  MongoDb as the transits service databaseTransits serviceThis micro-service is responsible for CRUD operations on train entities.// Transit document schema  {       transitId: string,       trainName: string,       start: string,       destination: string,       day: string,       departure: number,       arrival: number,       availableSeats: number,       lockedSeats: number,       totalSeats: number  }As the database, we will be using MongoDB Atlas pay as you go service in the GCP marketplace After configuring the MongoDb instance, let‚Äôs create the transits function.gostep service init transits version \"0.1.0\" env nodejsThis will create a boilerplate NodeJs cloud function in {PROJECT_ROOT}/src/transits and it can be executed as a http request after the deployment.Now let‚Äôs include the dependencies.cd src/transitsnpm install --save mongodbAfter creating the transits database and the collection, we can add MongoDb connection URI and collection name in the src/trains/functions.json as an environment variable.\"environmentVariables\": {    \"DB_URI\": \"mongodb+srv://&lt;username&gt;:&lt;password&gt;@&lt;your-cluster-url&gt;/&lt;dbname&gt;\",    \"COLLECTION\": \"Transits\"    },First let‚Äôs use these environment variables and create a function to connect to the database.import { MongoClient } from \"mongodb\"; const DB_URI = process.env.DB_URI || \"&lt;Default DB con URI&gt;\"; const dbClient = new MongoClient(DB_URI); const initDbClientConnection = async () =&gt; {    try {        await dbClient.connect();    } catch(e) {        console.error(e);        throw new Error(\"Database failed to connect!\");    }};And now let‚Äôs write 2 functions to find transits documents and save/update documents.const COLLECTION = process.env.COLLECTION || \"Transits\";const query = async (queries) =&gt; {    try {        await initDbClientConnection();        const transits = dbClient.db().collection(COLLECTION);        return await transits.find(queries).toArray();    } catch (e) {        console.error(e);        throw new Error(\"Failed to query transits!\")    } finally {        await dbClient.close();    }}const save = async(transitId, patches) =&gt; {    try {        await initDbClientConnection();        const transits = dbClient.db().collection(COLLECTION);        const targetData = { \"$set\": patches };        await transits.updateOne({ transitId: transitId }, targetData, { upsert: true });    } catch(e) {        console.error(e);        throw new Error(\"Failed to update transits!\");    } finally {        await dbClient.close();    }}In the main function we map GET and PUT https methods to above functions.export const main = async (req, res) =&gt; {    if(req.method === \"GET\") {        res.json(await query(req.query));    } else if(req.method === \"PUT\") {        const transitId = req.query[\"transitId\"];        if(!transitId) {            res.status(400).send({ \"error\": \"Invalid parameters!\" })        }        await save(transitId, req.body);        res.status(201).send();    } else {        res.status(400).json({ \"error\": \"Invalid request\" });    }}Great! Now we have our transits service. We can deploy it by running below command in the project root,gostep deploy diffAfter the deployment, transits service can be executed using http requests. But the endpoint is not available for the public. To test it locally, use the bearer token which you can obtain using the Gcloud cli.gcloud auth print-identity-tokenReservations serviceNext, we are going to implement the entrypoint of the global transaction. Like before, let‚Äôs bootstrap a cloud function again. Run,gostep service init reservations version \"0.1.0\" env nodejsNow we have our boilerplate code in {PROJECT_ROOT}/src/reservations.Considering this scenario the reservations function is responsible for,  Get the user request via a HTTP request.  Call transits service and find out if there is any transit avaialable.  If a transit is avialable publish an message to the relavent topic.  Save the event data with it‚Äôs status as ‚ÄòIN_PROGRES‚Äô, to update later.We are going to keep the event data stored in a database. So that we can keep the status of the particular event to use later. For that purpose we use Google Cloud firestore(data store in native mode), which is a serverless easy to use document database.To enable Firestore run,gcloud firestore databases create --region=asia-southeast1After that let‚Äôs install the dependencies. In the function root({PROJECT_ROOT}/src/resrevations) run,npm install --save \"@google-cloud/firestore\" \"@google-cloud/pubsub\"Let‚Äôs assume below payload as the request JSON.{    \"day\": \"Monday\",    \"start\": \"Colombo\",    \"destination\": \"Ragama\",    \"numberOfSeats\": 10,    \"userId\": \"xyz@gmail.com\"}Once the user made his request we have to obtain the available transits for the requested day, start position and destination of the transit. To do that we will using a HTTP request to the transits service we implemeted before. Since the transits APIs are not publically available we have to use the google-auth-library to authorize requests from other services(See more). There is no need to add the auth library as a dependecy since it is an already included library in the cloud function runtime.First let‚Äôs add an environment variable for the transits API endpoint in {PROJECT_ROOT}/src/transits/function.json.    \"environmentVariables\": {        \"TRANSITS_API\": \"{HOST_ADDRESS}/transits\"    }After that let‚Äôs authorize our request to fetch available transits.import { GoogleAuth } from \"google-auth-library\";const TRANSITS_API = process.env.TRANSITS_API || \"{DEFAULT_TRANSITS_HOST_ADDRESS}/transits\";export const getAvailableTransits = async (numberOfSeats, day, destination, start) =&gt; {    try{        // Create an authorized client to invoke restricted Transits API.        const auth = new GoogleAuth();        const transitsApiClient = await auth.getIdTokenClient(TRANSITS_API);        const result = await transitsApiClient.request({            url: `${TRANSITS_API}?day=${day}&amp;destination=${destination}&amp;start=${start}`,            method: \"GET\"        });        return result.data.filter(element =&gt; element[\"availableSeats\"] &gt;= numberOfSeats);    } catch(e) {        console.error(e);    }};Based on the result of the API request, we proceed further. Let‚Äôs assume that we got a list of available transits and we selected the topmost transit. Now we will be saving the event data in firestore, with a unique Id(a generated UUID as correlationId) as the global transation Id to identify the local transactions group and the status of the current event. It will aid to identify the local transaction for later references.Same as before we can add the firestore collection name(reffered as kind in firestore) of the event as an environment varibale in {PROJECT_ROOT}/src/transits/function.json.    \"environmentVariables\": {        \"EVENT_DATA_COLLECTION\": \"reservations\"    }Now we can write our function to save event data in firestore. Please note that you don‚Äôt have to include configurations to authorize the connection to the firestore since the cloud function runtime has the authorized access to the firestore in the same project.import Firestore from \"@google-cloud/firestore\";const EVENT_DATA_COLLECTION = process.env.EVENT_DATA_COLLECTION || \"reservations\";export const saveEvent = async (id, eventData) =&gt; {    try {        const firestore = new Firestore();        const docRef = firestore.collection(EVENT_DATA_COLLECTION).doc(id);        await docRef.set(eventData, { merge: true });        const result = await docRef.get()        return result.exists? result.data(): {}; // return the updated doc for later references    } catch(e) {        console.error(e);        throw new Error(\"Error saving event data!\");    }};And since we have assumed that we have an available transit, we are going to publish a message to a pubsub topic to trigger the next event, bookings. First let‚Äôs create a topic for this purpose.gcloud pubsub topics create reservations.bookingsAnd please copy the output of that command and keep it saved, we are going to need it later. Same as before let‚Äôs have another environment varible for the topic name and wirte the function to publish the message. In the message we include correlationId, numberOfSeats, transitId and userId.import { PubSub } from \"@google-cloud/pubsub\";const BOOKINGS_TOPIC = process.env.BOOKINGS_TOPIC || \"reservations.bookings\";export const publishMessage = async (topic, message) =&gt; {    try {        const pubsubClient = new PubSub();        const dataBuffer = Buffer.from(JSON.stringify(message));        return await pubsubClient.topic(topic)            .publishMessage({ data: dataBuffer });    } catch (e) {        console.error(e);        throw new Error(`Error publishing message to ${topic}!`);    }}Now we have all helper functions and we can write the logic in the main funtion. Once the function is complted to deploy run,gostep deploy diffBookings serviceThe bookings function is responsible for hold the requested number of seats in selected transit until the global transaction is finished. The acting trigger of the function will be the reservations.bookings pubsub topic we create during the previous step. Once this service successfully locked the request number of seats it will publish a message to the relevenat pubsub topic to trigger the payments function.Let‚Äôs start. To initialize the function run,gostep service init bookings version \"0.1.0\" env nodejs trigger pubsubNow we have bootstrapped our cloud function in {PROJECT_ROOT/src/bookings. Let‚Äôs tell the funtion that it will triggered by the reservations.bookings topic. For that we can include the resource value we copied from the previous topic creation in the {PROJECT_ROOT/src/bookings/function.json.    \"eventTrigger\": {        \"eventType\": \"providers/cloud.pubsub/eventTypes/topic.publish\",        \"resource\": \"projects/{GCLOUD_PROJECT_ID}/topics/reservations.bookings\"    }Same as the reservations function, we need to save the local transaction‚Äôs event data with the correlationId and the status as IN_PROGRESS for later references. Also we can use same functions from the previous service to authorize requests to transits service and to publish the message to the next topic. What we can do is update the transit document to lock the requested number of seats.import { GoogleAuth } from \"google-auth-library\";const TRANSITS_API = process.env.TRANSITS_API || \"{TRANSITS_API_HOST}/transits\";export const getTransitsById = async (transitId, client) =&gt; {    try {        const result = await client.request({            method: 'GET',            url: `${TRANSITS_API}?transitId=${transitId}`        });        return result.length &gt; 0? result[0]: {};    } catch (e) {        console.error(e);        throw new Error(`Error fetching transit data: ${transitId}`);    }};export const updateTransitsById = async (id, newData, client) =&gt; {    try{        return await client.request({            method: \"PUT\",            url: `${TRANSITS_API}?transitId=${id}`,            body: newData        });    } catch(e) {        console.error(e);    }};export const main = async (eventData) =&gt; {    const transactionData = JSON.parse(atob(eventData.data)); // Extract data from pubsub message    const correlationId = transactionData[\"correlationId\"];    const numberOfSeats = Number(transactionData[\"numberOfSeats\"]);    const transitId = transactionData[\"transitId\"];    const userId = transactionData[\"userId\"];    const auth = new GoogleAuth();    const transitsApiClient = await auth.getIdTokenClient(TRANSITS_API);    const transit = await getTransitsById(transitId, transitsApiClient);    await updateTransitsById(transitId, {                \"lockedSeats\": transit[\"lockedSeats\"] + numberOfSeats,                \"availableSeats\": transit[\"availableSeats\"] - numberOfSeats                }, transitsApiClient);}And like before we will be creating the next pubsub topic to publish the message from bookings.gcloud pubsub topics create reservations.paymentsAfter a successful seat locking, we will be publishing a message with correlationId, numberOfClients and userId.Once the function has been completed we can deploy it using,gostep deploy diffGreat! Now we have covered common functionalities,  Consume HTTP requests  Function to function direct communication(via HTTP)  Read and update event data in firestore  Publishing and subscribing to Pubsub topicsThis is more than enough for us to implement next services. Therefor afterwards, I will be explaining the function‚Äôs role only.Payments serviceThe payments service will consume the message from reservations.payments and publish a message to reservations.bookingCompletions or reservations.bookingCancelletions accordingly for a successful payment or for a failed payment.Booking completions serviceThe booking completions will be consuming the messages from reservations.bookingCompletions topic, will be update the transit as the seat booking is completed and after that will update previously saved booking event‚Äôs status from IN_PROGRESS to COMPLETED for the relevant correlationId. Then the service will publish an message to the reservations.reservationCompletions topic.Booking cancellations serviceIn the event of a payment failure, after consuming the message from the topic reservations.bookingCancelletions this function will rollback the locked seats in the relevant transit, will update booking event‚Äôs status from IN_PROGRESS to FAILED for the relevant correlationId and will pass the correlationId to the reservations.reservationCancellations topic.Reservation completions serviceAs the final step of a completed series of events the reservation completions service will consume the correlation id for the transaction from reservations.reservationCompletions and will update previously saved reservations event‚Äôs status from IN_PROGRESS to COMPLETED. After that a message will be published to the reservations.notifications topic to send the successful transaction notifications to the customer.Reservation cancellations serviceConsuming the message from reservations.reservationCancellations this function will update previously saved reservations event‚Äôs status from IN_PROGRESS to FAILED and will publish a message to the reservations.notifications topic to send the failed transaction notifications status to the customer.Securing the entrypointAfter deploying all the services we can use Google API gateway to secure our reservations entrypoint of the transaction.Please refer API gatewey quickstart.ü¶ñ Let‚Äôs look into Orchestration based solution in the next article."
  },
  
  {
    "title": "Super charge a Google cloud functions project",
    "url": "/posts/gostep-intro/",
    "categories": "Projects, DevOps",
    "tags": "gcp, serverless, gostep, python, cli",
    "date": "2020-06-19 00:00:00 +0530",
    





    
    "snippet": "When developing a microservices project with cloud functions, managing the cluster of functions all of them together can be a pain in the ass. That is why I thought of developing a simple cli tool ...",
    "content": "When developing a microservices project with cloud functions, managing the cluster of functions all of them together can be a pain in the ass. That is why I thought of developing a simple cli tool to super charge the development and deployment process.I named this little Pythonic tool as gostep a.k.a serverless templates provider for Google cloud platform. However this tool is still taking the baby steps. Hope to develop this to be more useful in future releases.I would like to show you how it works up to now.First of all‚Ä¶You need to have installed below components to use gostep cli.  Python version 3.x with PyPI a.k.a pip(https://www.python.org/download/releases/3.0/, https://pypi.org/project/pip/)  Gcloud sdk(https://cloud.google.com/sdk)  subversion(https://subversion.apache.org)  gostep(https://github.com/gostep-cli/gostep)Next steps‚Ä¶Now simply install gostep cli.pip install gostepAfter installing gostep, using gcloud sdk log in to your google cloud platform account.gcloud auth loginOnce you logged in, select the gcloud project that you want to use for your serverless functions.gcloud config set project {projectId}Oh! wait, to list down project Ids gcloud projects list or gostep gcloud projects can be used.All set‚Ä¶Now we are ready build a cloud functions cluster.First gostep needs a workspace directory, a gcloud service account and a credentials file for deployment purposes.We can initiate them by this command, gostep auth init {new_service_account_name} inside {workspace_directory}.gostep auth init my-service-account inside ./my-workspaceNow we can see a credentials file has been generated inside the workspace.Next we need to create a configuration file which keeps the project skeleton. We need to chose a default region for that. Otherwise gostep will choose that for us. To get a list for our gcloud project gostep gcloud locations can be used. Now we can simply do,gostep base init {project_name} location{gcloud_region_id} version \"0.1.0\" explains {description} inside {workspace_directory}In our case,cd my-workspacegostep base init my-new-project location asia-east2 version \"0.1.0\" explains \"my sample project\"It‚Äôs geen light now to create cloud functions now. gostep has specified template structures for this.Let‚Äôs simply bootstrap a python cloud function. For this purpose,gostep service init {cloud_function_name} location {gcloud_region_id} version {service_version} env {runtime} explains {desciption} inside {workspace_directory}Since we are in the workspace directory and we already set up a default location Id we won‚Äôt be using location and inside arguments.gostep service init my-python-function version \"0.1.0\" env pythonLet‚Äôs bootstrap another function with nodeJs.gostep service init my-nodejs-function version \"0.1.0\" env nodejsWe can see source files inside the {workspace_directory}/src. In our case inside, my-workspace/src/my-nodejs-function and my-workspace/src/my-python-function.Great‚Ä¶!Now our project is ready to get deployed.Aight‚Ä¶ Let‚Äôs do,gostep deploy diffdiff keyword will only deploy the changes we made for our functions(gostep tracks md5 of the function directory). To deploy a single function it needs to be called by name. gostep deploy {function_name}. In our case,gostep deploy my-nodejs-functionBravo‚Ä¶!Now our functions are deployed and ready to be executed.In future releases‚Ä¶  More templates, templates for go lang, templates for Spring framework, etc‚Ä¶  Handle function triggers such as pubsub, events, etc‚Ä¶  Run cloud functions cluster in local environment, so developers can benefit debugging.Please find the source code in https://github.com/gostep-cli/gostep."
  },
  
  {
    "title": "Application Deployment in Apache Tomcat on GCE Using Ansible",
    "url": "/posts/tomcat-gce-ansible-demo/",
    "categories": "Posts, DevOps",
    "tags": "tomcat, gce, ansible, configuration-management",
    "date": "2018-03-01 00:00:00 +0530",
    





    
    "snippet": "Think about a person who needs a cloud instance temporarily to deploy a web application to do tests frequently and throughout the time he deploys the application,use it for a while and then deletes...",
    "content": "Think about a person who needs a cloud instance temporarily to deploy a web application to do tests frequently and throughout the time he deploys the application,use it for a while and then deletes the instance to save the cost.Or someone needs to create a cluster, thus he needs to instantiate several cloud servers at once,install dependencies and deploy the application on each server. Doing these tasks by hand costs much effort and it is inefficient.The way to make such scenarios easier, efficient and effective is making a reusable structure which does these repetitive tasks when we invoke it.For that purpose, we use configuration management.This tutorial is about a such scenario, to splash the easiness of using a reusable code base when deploying an application,using Ansible(a radical and impressive configuration management tool with its capabilities and ease of use compared to other tools),Google cloud platform(future potential cloud services with a good pricing model) and Apache Tomcat(one of the most popular web servers in the Java community). For this purpose, we are going to use the gce-tomcat-ansible-demo repository, a concatenation of Ansible playbooks(the reusable code base) implemented by me to reflect this task.  Running this will create a Google compute engine in a given Google cloud platform project, install java,   configure an Apache Tomcat server and will deploy a war file according to given metadata.   (To understand playbooks knowing Ansible basics is more than enough. Refer Ansible documentation)Getting startedTo run the playbook you need to have a Google cloud platform account, a Google cloud platform project and a service accountto manipulate Google cloud project with appropriate roles and permission and an Ansible running machine.There are several ways to install Ansible but here let‚Äôs install it using python pip package manager since the installationis not going to depend on which operating system you use. But first make sure Python version 2,Python development and Python pip(most probably python-dev and python-pip respectively,refer installing pip with package managers for more information) packages have been installed on the machine that you are going to install Ansible. To make sure, try running the command below.pip listAnd then clone the repository gce-tomcat-ansible-demo, which contains playbooks to create a Google compute engine instance, install Java, configure a Apache Tomcat server and deploy a given war file on the configured application server.git clone https://github.com/lpsandaruwan/gce-tomcat-ansible-demo.gitThen change the current working directory into the cloned repository.cd gce-tomcat-ansible-demoNow use the requirements.txt to install appropriate Python pip package versions which this playbook has been written and tested for.pip install -r requirements.txtThis will install ansible and apache-libcloud(a fine interface to deal with popular cloud services)Python packages which we are going to use for manipulating Google cloud project.Make sure that you have a working Google cloud platform project(if not refer creating and manage projects),and a service account assigned to it(do not use the default service account since it has full permission over the project,refer service accounts for more information) and then obtain the project ID,private JSON keyfile(you can obtain the JSON key file when creating a new service account,if not refer service account credentials) and the service account email from them.Now we have to configure Ansible running machine to access the GCP project.Here Let‚Äôs use Google cloud SDK to make things easier(refer install Google cloud SDK).After installing the SDK run the below command to initialize.gcloud initAnd it will direct you to the web page in your browser. From there allow the access to the SDK. Now in the terminal select the appropriate project ID. After that you should be able to run playbooks on the appropriate project and manipulate it.  If you run into a permission problem connecting the instance configure SSH authentication using cloud SDK tools by running the command below.  (refer gcloud compute config-ssh)gcloud compute config-sshPlay itNow configure the file gce-vars/authentication and update the obtained metadata from GCP project and service account in playbook.project_id: ‚Äúproject-id-193706‚Äùcredentials_file: ‚Äú/path/to/private/json/key/file‚Äùservice_account_email: ‚Äútomcat-ansible-demo@service-account-193706.iam.gserviceaccount.com‚ÄùAfter that change instance metadata in gce-vars/instance as you need. Here,we are going to add firewall rules to allow HTTP traffic on 8080 ports for Apache Tomcat servername: tomcat-ansible-demotype: f1-microimage: debian-9zone: europe-west1-ballowed_ports_tcp: tcp:8080allowed_ports_udp: udp:8080Then set the ANSIBLE_HOSTS environment variable required by Ansible for SSH interactions.To do that simply put hostnames in hosts file and import it as below.export ANSIBLE_HOSTS=hostsNow you should be able to run this project. Simply run the main playbook.ansible-playbook run.ymFinal output will be as below. And after a successful run you will have your application deployed in anApache Tomcat server on a Google compute engine instance.Appendix(If you wish to change Java version, Tomcat version etc.configure main.yml in defaults directory in roles. They contain configuration variables with lower priorities.)gce-tomcat-ansible-demo|------gce_vars\t\t\t\t# variables related to Google cloud platform|\t|\tauthentication\t\t# Google project and service account related metadata|\t|\tinstance\t\t# GCE instance related metadata||-------roles|\t|-------java\t\t\t\t# role to install Java|\t|\t|-------defaults|\t|\t|\t|\tmain.yml\t# default variables for java role|\t|\t||\t|\t|-------tasks|\t|\t|\t|\tmain.yml\t# tasks to download and install Java|\t||\t|-------tomcat|\t|\t|-------defaults|\t|\t|\t|\tmain.yml\t# default variables for tomcat role|\t|\t||\t|\t|-------files|\t|\t|\t|\ttomcat-users.xml\t# set tomcat manager credentials|\t|\t||\t|\t|-------tasks|\t|\t|\t|\tmain.yml\t# tasks to download and configure tomcat|\t||\t|-------tomcat-deploy|\t|\t|-------defaults|\t|\t|\t|\tmain.yml\t# default variables for tomcat-deploy role|\t|\t||\t|\t|-------tasks|\t|\t|\t|\tmain.yml\t# tasks to deploy the given war file||\thosts\t\t\t\t\t# ansible hosts|\tbootstrap-instance.yml\t\t\t# playbook to initiate google cloud instance|\tdeploy-war.yml\t\t\t\t# playbook to deploy war file|\tinstall-java.yml\t\t\t# playbook to install Java|\tinstall-tomcat.yml\t\t\t# playbook to install Apache Tomcat|\trun.yml\t\t\t\t\t# main playbook to rungce-tomcat-ansible-demo post by Lahiru Pathirage is licensed under a Creative Commons Attribution 4.0 International License.Based on a work at https://github.com/lpsandaruwan/gce-tomcat-ansible-demo."
  },
  
  {
    "title": "Setting up Creative Labs USB DAC volume knob on Linux",
    "url": "/posts/sb1095-volume-knob-linux/",
    "categories": "Posts, Linux",
    "tags": "usb dac, creative labs, sb1095, linux, volume knob",
    "date": "2017-05-11 00:00:00 +0530",
    





    
    "snippet": "Lately I bought a Creative Labs SB1095, a 5.1 USB DAC for my laptop.This USB sound card works perfectly on Linux.The sound quality is better than the integrated,but it has a volume knob on it and a...",
    "content": "Lately I bought a Creative Labs SB1095, a 5.1 USB DAC for my laptop.This USB sound card works perfectly on Linux.The sound quality is better than the integrated,but it has a volume knob on it and a remote controller,which does not support out of the box by Linux distributions which I have tried(Ubuntu, Linux Mint, OpenSuse, Arch Linux).After digging up the internet a little bit I perceived that it is required to configure a software volume controller to handle the volume knob.However, I did not want to go for that kind of advanced configurations, and finally found an easy workaround for this purpose. Would like to take down the steps,so it might help others which have the same issue.To achieve this I used lirc (an application which interprets IR actions) to detect volume knob and remote actions and wrapped it with irexec (trigger actions for lirc inputs) to change system sound volume.I am currently using Linux Mint 18.1, so this solution will work perfectly with Ubuntu 16.04 derivatives.For other Linux distributions please follow this guide.RequirementsFirst I installed these packages, selected Creative USB IR Receiver (SB0540) for remote controller configuration(not the SB1095,but it has the same configurations) and selected none for IR transmitter in appearing configuration menus when installing.sudo apt install lirc lirc-xConfigurationsThen I changed the REMOTE_DRIVER in lirc hardware configuration file /etc/lirc/hardware.conf, left other settings unchanged.# ~/.lircrcbegin remote = * prog = irexec config = amixer -D pulse sset Master 5%- button = vol- repeat = 1endbegin remote = * prog = irexec config = amixer -D pulse sset Master 5%+ button = vol+ repeat = 1endbegin remote = * prog = irexec config = amixer -D pulse sset Master 0 button = muteendTestThen I restarted lirc daemon and loaded irexec daemon.sudo service lirc restartirexec -d # make a startup entry to load on system boot upNow I have the volume knob working just fine.If the volume is not changing check whether lirc detects inputs by the USB device using the command irw# irw sample output for volume knob changes0000000000000010 01 vol+ RM-15000000000000000010 00 vol+ RM-15000000000000000010 01 vol+ RM-1500000000000000000f 03 vol- RM-1500000000000000000f 00 vol- RM-1500000000000000000d 00 mute RM-1500If the output is something like the above, try changing config values(commands to change sound volume) in the ~/.lircrc after that try restarting lirc daemon and loading irexec again.Sourceshttp://alsa.opensrc.org/Usb-audio#Creative_USB_X-Fi_Surround_5.1http://www.lirc.org/html/configure.html"
  },
  
  {
    "title": "Continuous Code Quality On My OpenSource Project",
    "url": "/posts/github-sonarqube/",
    "categories": "Posts, DevOps",
    "tags": "code-quality, github, travis-ci, sonarqube",
    "date": "2017-04-20 00:00:00 +0530",
    





    
    "snippet": "Good quality in code plays an essential role when it comes to software,thus it assets efficiency, reliability, robustness, portability, maintainability and readability like essential factors.Consid...",
    "content": "Good quality in code plays an essential role when it comes to software,thus it assets efficiency, reliability, robustness, portability, maintainability and readability like essential factors.Considering a GitHub project, there are plenty of options to measure code quality.Considering options I would like to chose SonarQube for this particular purpose.Let me take down the steps, how I used SonarQube to measure code quality using a Java project, one of my GitHub hosted projects, Depli.NB: The best way to analyze a maven project is to use the maven sonar plugin as the SonarQube docs says.You do not require a sonar-project.properties in that case.Step 1 - Create an account in travis-ci.orgSonarQube needs sonar-runner to analyze the code.To run the analysis process using sonar-runner on code changes continuously, the ideal solution is using a CI server.Here I had to use Travis-CI since it is the perfect matured CI solution for GitHub projects.I created and logged into Travis using my GitHub account and activated it for my repository.Step 2 - Create an account in sonarqube.comThen I created and logged into SonarQube using the same GitHub account.Step 3 - Create Travis-CI configuration fileNext step was to create a configuration file for Travis-CI to instruct it to how to run the sonar-scanner as known as sonar-runner.To do that I created a .travis.yml, A YAML file in my project‚Äôs root directory.dist: trusty # chose ubuntu trusty as the workersudo: requiredaddons:  sonarqube:    organization: lpsandaruwan-github # organization token from https://sonarqube.com/account/organizationsjdk:- oraclejdk8script:- mvn clean install -DskipTests # skipped tests because I have not written.- sonar-scanner # tell travis to run sonar scannercache:  directories:  - \"$HOME/.sonar/cache\"step 4 - Create a SonarQube tokenAfter creating a Travis configuration file I generated a security token and copied it to clipboard,for Travis to use when updating SonarQube database.Step 5 - Encrypt SonarQube tokenPublic access to a security token is a bad thing. So I had to encrypt the SonarQube token when inserting it to Travis configuration file.To achieve that I used travis from ruby gems.cd /path/to/my/project/roottravis encrypt MY_SONARQUBE_TOKEN --add addons.sonarqube.tokenStep 6 - Create a SonarQube configuration fileA metadata file including project details is required for SonarQube. So I created sonar-project.properties in my project‚Äôs root.Here sonar.sources is the place where sonar-scanner starts to analyze.sonar.projectKey=com.sonarqube.lpsandaruwan.deplisonar.projectName=Depli - JVM Monitoring Dashboardsonar.projectVersion=0.2.0-SNAPSHOTsonar.links.homepage=https://lahirus.com/deplisonar.links.ci=https://travis-ci.org/lpsandaruwan/deplisonar.links.scm=https://github.com/lpsandaruwan/deplisonar.links.issue=https://github.com/lpsandaruwan/depli/issuessonar.sources=src/mainStep 7 - Add status to read meNow to display the project status on readme, I added labels from Travis and SonarQube on README.md file.[![Build Status](https://travis-ci.org/USERNAME/PROJECT_NAME.png)](https://travis-ci.org/USERNAME/PROJECT_NAME)[![Quality Gate](https://sonarqube.com/api/badges/gate?key=SONAR_PROJECT_KEY)](https://sonarqube.com/dashboard/index/SONAR_PROJECT_KEY)Bravo!After following above steps I pushed all changes to GitHub. And waited until Travis sent me a mail confirming that my build has been successful.Now the readme is displaying the build status and whether my project has passed the quality gate.By clicking on the quality gate badge, I can access the SonarQube dashboard, detailed analysis of code quality of my repository.Please refer my open source project, https://github.com/lpsandaruwan/depli if there is any doubt."
  },
  
  {
    "title": "Continuous listening to remote text files using python",
    "url": "/posts/log-tracker/",
    "categories": "Projects, DevOps",
    "tags": "python, log files, paramiko, wrapper application",
    "date": "2017-04-14 00:00:00 +0530",
    





    
    "snippet": "Log Tracker is a simple wrapper around Python paramiko to track text files using SSH.It gives you the ability to create custom python functions to track and analyze log files the way you want.A use...",
    "content": "Log Tracker is a simple wrapper around Python paramiko to track text files using SSH.It gives you the ability to create custom python functions to track and analyze log files the way you want.A user can access the contents in multiple log files at the same time also.Using a custom function a user can display a log content in a web interface using Flask like lightweight web service,so then anyone can analyze contents easily, without wasting time to login into servers and download contents.For more information please refer below links.Website: https://lahirus.com/log-trackerWiki: https://github.com/lpsandaruwan/log-tracker/wikiLicense: GPLv3 or laterSource code: https://github.com/lpsandaruwan/log-trackerReleases: https://github.com/lpsandaruwan/log-tracker/releases"
  },
  
  {
    "title": "Depli - A JVM monitor application",
    "url": "/posts/depli/",
    "categories": "Projects, DevOps",
    "tags": "java, spring-boot, angularjs, jvm performance",
    "date": "2017-04-03 00:00:00 +0530",
    





    
    "snippet": "Depli provides you the 10-second solution for monitoring JVMs. Just add a JMX remote connection using webUI and see how it works.Depli provides you a rich UI, you can even search for running thread...",
    "content": "Depli provides you the 10-second solution for monitoring JVMs. Just add a JMX remote connection using webUI and see how it works.Depli provides you a rich UI, you can even search for running threads, classpaths etc.This handsome tool has been released under GPL license on GitHub.Website: https://lahirus.com/depliWiki: https://github.com/lpsandaruwan/depli/wikiLicense: GPLv3 or laterSource code: https://github.com/lpsandaruwan/depliReleases: https://github.com/lpsandaruwan/depli/releases"
  },
  
  {
    "title": "JVM CPU usage using Java MXBeans",
    "url": "/posts/jvm-cpu-usage/",
    "categories": "Posts, Other",
    "tags": "ava, jvm cpu utilization, mxbeans, devops",
    "date": "2017-02-26 00:00:00 +0530",
    





    
    "snippet": "This is a solution to a problem, occurred to me while developing Depli a JVM monitoring dashboard which uses JMX remote connections.There is no way to get the JVM CPU usage directly using MXBeans i...",
    "content": "This is a solution to a problem, occurred to me while developing Depli a JVM monitoring dashboard which uses JMX remote connections.There is no way to get the JVM CPU usage directly using MXBeans in JDKs older than version 7. For my application I wanted a universal method.Finally, I got it working thanks to source code of jconsole.ExplanationIdeal solution to calculate CPU usage is periodically look at the idle time and get the time that JVM is not idle.But there is no method to expose idle time in MXBeans. So here, JVM CPU usage is calculated using getting theratio of how much discrete time slices JVM used and how long JVM was up while using those time slices(total JVM uptime in all the available threads for JVM),in a particular time period. Below algorithm will explain it better.previousJvmProcessCpuTime = 0;previousJvmUptime = 0;function getCpuUsagePercentage() {    elapsedJvmCpuTime = currentJvmCputime - previousJvmProcessCpuTime;    elapsedJvmUptime = currentJvmUptime - previousJvmUptime;    // total jvm uptime    totalElapsedJvmUptime = elapsedJvmUptime * availableNumberOfCpusForJvm;    // calculate cpu usage ratio    cpuUsage = (elapsedJvmCpuTime / totalElapsedJvmUptime);    previousJvmProcessCpuTime = currentJvmCputime;    previousJvmUptime = currentJvmUptime;    // return as a percentage    return cpuUsage * 100;}Get it workingWe will be using remote method invocation(RMI) to connect and call methods in remote MXBean interfaces.First we have to connect to a JMX remote connection and have to initiate a managed beans server connection.// hardcoded connection parametersfinal String HOSTNAME = \"localhost\";final int PORT = 9999;// initiate address of the JMX API connector serverString serviceURL = \"service:jmx:rmi:///jndi/rmi://\" + HOSTNAME + \":\" + PORT + \"/jmxrmi\";JMXServiceURL jmxServiceURL = new JMXServiceURL(serviceURL);// initiate client side JMX API connector// here we set environment attributes to null, since I am not using any authentication method connect to JMX remoteJMXConnector jmxConnector = JMXConnectorFactory.connect(jmxServiceURL, null);// initiate managed bean server connectionMBeanServerConnection mBeanServerConnection = jmxConnector.getMBeanServerConnection();Now we can create proxy connections to MXBean interfaces to forward method calls though the managed bean serverconnection that we have created above. We will be using runtime MXBean, operating system MXBean and operating systemMXBean from platform extension.com.sun.management.OperatingSystemMXBean peOperatingSystemMXBean;java.lang.management.OperatingSystemMXBean operatingSystemMXBean;java.lang.management.RuntimeMXBean runtimeMXBean;// initiate proxy connectionspeOperatingSystemMXBean = ManagementFactory.newPlatformMXBeanProxy(        mBeanServerConnection,        ManagementFactory.OPERATING_SYSTEM_MXBEAN_NAME,        com.sun.management.OperatingSystemMXBean.class);operatingSystemMXBean = ManagementFactory.newPlatformMXBeanProxy(        mBeanServerConnection,        ManagementFactory.OPERATING_SYSTEM_MXBEAN_NAME,        OperatingSystemMXBean.class);runtimeMXBean = ManagementFactory.newPlatformMXBeanProxy(        mBeanServerConnection,        ManagementFactory.RUNTIME_MXBEAN_NAME,        RuntimeMXBean.class);We can call the appropriate methods to get required data now. We need data on how long JVM processes used the CPU, JVM uptime andhow much processors allowed for the JVM in a paticular time period.// keeping previous timestamplong previousJvmProcessCpuTime = 0;long previousJvmUptime = 0;// Get JVM CPU usagepublic float getJvmCpuUsage() {    // elapsed process time is in nanoseconds    long elapsedProcessCpuTime = peOperatingSystemMXBean.getProcessCpuTime() - previousJvmProcessCpuTime;    // elapsed uptime is in milliseconds    long elapsedJvmUptime = runtimeMXBean.getUptime() - previousJvmUptime;    // total jvm uptime on all the available processors    long totalElapsedJvmUptime = elapsedJvmUptime * operatingSystemMXBean.getAvailableProcessors();        // calculate cpu usage as a percentage value    // to convert nanoseconds to milliseconds divide it by 1000000 and to get a percentage multiply it by 100    float cpuUsage = elapsedProcessCpuTime / (totalElapsedJvmUptime * 10000F);    // set old timestamp values    previousJvmProcessCpuTime = peOperatingSystemMXBean.getProcessCpuTime();    previousJvmUptime = runtimeMXBean.getUptime();    return cpuUsage;}Now we can get JVM CPU usage by calling the getJvmCpuUsage periodically. You can obtain the complete Java code from this Gist."
  },
  
  {
    "title": "Savior ship - A cross flatform C++ game",
    "url": "/posts/savior-ship/",
    "categories": "Projects, Gaming",
    "tags": "c++, sdl2, game, 2d",
    "date": "2017-02-23 00:00:00 +0530",
    





    
    "snippet": "Savior Ship is a simple 2D shooter game implemented using C++ and simple direct media layer version 2(SDL2),This is the way I attempted to refresh my skills on C++. Savior ship has been released un...",
    "content": "Savior Ship is a simple 2D shooter game implemented using C++ and simple direct media layer version 2(SDL2),This is the way I attempted to refresh my skills on C++. Savior ship has been released under GPL license in GitHub for the purposeof helping out the people who seek SDL2 beginner level applications to learn.Website: https://lahirus.com/savior-shipLicense: GPLv3 or laterSource code: https://github.com/lpsandaruwan/savior-shipReleases: https://github.com/lpsandaruwan/savior-ship/releases"
  },
  
  {
    "title": "Surgeon Tux - A Jekyll template",
    "url": "/posts/surgeon-tux/",
    "categories": "Projects, Other",
    "tags": "jekyll, template, monospace, dark",
    "date": "2017-02-23 00:00:00 +0530",
    





    
    "snippet": "Surgeon Tux is a GPL(v3) licensed free Jekyll template meant for terminal lovers.I tried my best to give it the appearance of a terminal.Where can I find the source code?Surgeon Tux code is hosted ...",
    "content": "Surgeon Tux is a GPL(v3) licensed free Jekyll template meant for terminal lovers.I tried my best to give it the appearance of a terminal.Where can I find the source code?Surgeon Tux code is hosted in a github repository, you can obtain it from https://github.com/lpsandaruwan/surgeon-tuxHow to use it?First clone the surgeon tux,git clone https://github.com/lpsandaruwan/surgeon-tux.gitcd surgeon-tuxMake sure you have installed RubyGems package manager,gem install jekyllgem install jekyll-paginateBefore running Jekyll, consider changing the configurations listed in _config.ymlTo run Surgeon Tux,jekyll serveNow log into http://localhost:4000 from your web browser.You can refer the demo here, https://lahirus.com/surgeon-tux"
  },
  
  {
    "title": "Traverse through font design timeline",
    "url": "/posts/kalagola/",
    "categories": "Projects, Other",
    "tags": "python, cli, fonts",
    "date": "2017-01-26 00:00:00 +0530",
    





    
    "snippet": "KalaGola is a cross platform Pythonic tool to traverse through the timeline of your GitHub hosted font repository,and generate a video file to show how your font evolved through time. Just provide ...",
    "content": "KalaGola is a cross platform Pythonic tool to traverse through the timeline of your GitHub hosted font repository,and generate a video file to show how your font evolved through time. Just provide your custom web template andrepository information as a YAML file or as command line arguments, KalaGola will serve you your video.How does it work?KalaGola uses weasyprint to capture html snapshots after checking out each and every commit made on a specificfont file and copying the font file on that state to the assets directory which html template uses. As soon as possible KalaGola finished capturing images,using OpenCV bindings for Python, KalaGola converts captured images to frames and then frames to a video file.How can I use it?Like mentioned above, KalaGola configurations can be given as a YAML file or as command line arguments. If no argument has been providedKalaGola uses config.yml file by default.Contents of config.yml filename: myfontassets_dir: template/assetsbranch: gh-pagesfont_file: fonts/AbhayaLibre-Regular.otfindex_file: template/index.htmlinterval: 100repository: abhaya-libre-fontstylesheet: template/assets/stylesheet.cssuser: mooniak      name - For name, provide any name you like(do not use spaces), once KalaGola has finished, you will find name.avi video file in the videos directory from KalaGola  home.        assets_dir - This is where you are going put your font while traversing commits. The font file copied to assets directory will be named as myFont and you shoulduse it in your stylesheet like this or in a better way,  @font-face {    font-family: \"My Font\";    src: url('assets/myfont') format(\"woff\");}      index_file - This is the template html file which contains the layout designs, designed using your font file, which is to be captured while changing the state of font file.        stylesheet - The stylesheet can be embedded into html file‚Äôs header or YAML file‚Äôs stylesheet field.        repository - This one stands for GitHub repository name.        user - Consider puttting GitHub user name or organization here.        branch - This is the branch of the repository which contains font files, which is affected by continuous integration process, where font files, ttf or otf files have been automatically built or updated, or simply have been built manually and pushed.        font_file - This is the path to the font file in the branch, which KalaGola refers to load the templates and capture images.        interval - This field is used to reduce the CPU utilization while KalaGola capturing snapshot, by default it is 100, which means for every 100 of captures,KalaGola sleeps for a second. For older PCs reduce the interval number so they won‚Äôt get heat up.  To get an idea about the project structure, please refer Abhaya Libre Font, which is mentioned in the default config.yml file of KalaGola.Command line argumentsSimply use command line arguments to override default settings in runtime. They are the same as the YAML config file‚Äôs options.Prefer kalagola -h for more information,usage: kalagola.py [-h] [-a ADIR] [-b BRANCH] [-f FONT] [-i INDEX] [-n NAME]                   [-r REPO] [-s STYLESHEET] [-t INTERVAL] [-u USER] [-y YAML]optional arguments:  -h, --help            show this help message and exit  -a ADIR, --adir ADIR  Directory to put font file dynamically  -b BRANCH, --branch BRANCH                        Branch containing font files  -f FONT, --font FONT  Font file source  -i INDEX, --index INDEX                        Custom index html file  -n NAME, --name NAME  Output file name  -r REPO, --repo REPO  GitHub user repository  -s STYLESHEET, --stylesheet STYLESHEET                        Custom CSS style sheet  -t INTERVAL, --interval INTERVAL                        Refresh intervals to increase CPU usage  -u USER, --user USER  GitHub username/organization name  -y YAML, --yaml YAML  Read settings from yaml fileHow can I get KalaGola?Download KalaGola releases from here, https://github.com/lpsandaruwan/kalagola/releasesYou also can obtain the source code from here, https://github.com/lpsandaruwan/kalagolaI have an issue. What can I do?Please feel free to report issues here, https://github.com/pathumego/kalagola/issuesCredits and contributors  Pathum Egodawatta  Lahiru PathirageLicenseKalaGola is relased under GPLv3. Anyone is permitted to do copy, edit or redistribute."
  }
  
]

