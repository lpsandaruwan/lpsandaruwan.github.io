[
  
  {
    "title": "Saga Pattern with serverless model on Google Cloud Platform - Part 1",
    "url": "/posts/saga-gcp-choreography/",
    "categories": "Posts, Serverless",
    "tags": "gcp, serverless, saga, choreography, gostep",
    "date": "2022-11-20 00:00:00 +0530",
    





    
    "snippet": "  Gostep: 👉 GuideMaterials: 👉 Complete source codeDuring the past few years, the microservices architecture(MSA) and serverless model have gained a lot of popularity in the industry. However, these...",
    "content": "  Gostep: 👉 GuideMaterials: 👉 Complete source codeDuring the past few years, the microservices architecture(MSA) and serverless model have gained a lot of popularity in the industry. However, these technologies come with their own set of challenges. One substantial challenge is managing data in MSA due to its complexity. Considering common patterns for MSA data management we will be focusing on the Saga pattern in this article.The Saga patternIn order to manage business transactions across multiple microservices, the Saga pattern was introduced. Basically it is a series of local transactions; every transaction happens within the boundary of the micro-service, which every service will publish an event after the transaction for the next subsequent micro-service to perform the next transaction consuming the published event. This process will continue till the last transaction. In case any transaction failed in this series Saga will execute a series of fallback actions to undo the impact of all previous transactions.There are two approaches to implementing the Saga pattern.      Choreography - The micro-service is responsible for emitting events eventually of its local transaction. The published event will trigger the execution of local transactions in microservices subscribed to the event. Also in this approach micro-service is responsible for handling the errors.        Orchestration - A central orchestrator(a stateful coordinator) will trigger the local transactions in services and will maintain the global transaction status including handling errors.  Now that we have a basic understanding of Saga pattern, we will discuss how to implement Saga pattern, defining an example for both approaches using Google Cloud Serverless model.The real world exampleLet’s consider a train ticket booking system.The workflow consists of,  Send a seat reservation request  Check for available transits in the database and proceed with seat booking.  Hold the number of seats until payment is processed.  Process the payment.  Confirm the seat booking.  Confirm the reservation and notify the customer.However if the system encountered any error while running a local transaction, the fallback sequence should be executed to undo all the changes happening in the global transaction to keep the ACID properties.Preparing the development environment  (Please note that we won’t be using a real payment gateway or a notification service, beacause the main purpose of this article is to demonstrate how to use severless model for Saga.)To implement the solution we will be using Google Cloud serverless services, MongoDb and Javascript.Before we begin we must have,  A billing enabled Google Cloud project  Prior knowledge in Google Cloud Services  Python, NodeJs, GCloud cli tools installed in your system(If you are using Windows, WSL might come in handy)Google Cloud CLI/Cloud consoleYou can use both CLI tools or web console to create and modify services. In this article we will be mostly using CLI tools. Please follow https://cloud.google.com/sdk/docs/install to install the Google Cloud SDK. And once you installed the SDK run gcloud init command and follow instructions to configure credentials.Building the Cloud functions project structureTo build the project structure and functions, we will be using gostep, a pythonic CLI tool that I created previously to manage implementations when there are a lot of cloud functions.To use gostep you need to have Subversion CLI, Python version 3 and Pip package manager installed(Setup a virtual environment of your own preference). When you are ready, run the command, pip install gostep. For more information please refer, http://lahirus.com/gostep-intro. Also please make sure that you have enabled Cloud build APIs(https://console.cloud.google.com/apis/library/cloudbuild.googleapis.com).Using gostep, let’s first create a Cloud Functions project.mkdir SagaChoreography &amp;&amp; cd SagaChoreographygostep auth init reservationsservice       # Creates a service account credentials filegostep base init reservations location asia-east2 version \"0.1.0\"   # Creates gostep project metadata files and directory structure.Now we can have the project base. Let’s move ahead with implementing local transactions and services.Choreography based solutionFor the demonstration we will be using,  Pub/Sub for event sharing  Firestore to store event data  MongoDb as the transits service databaseTransits serviceThis micro-service is responsible for CRUD operations on train entities.// Transit document schema  {       transitId: string,       trainName: string,       start: string,       destination: string,       day: string,       departure: number,       arrival: number,       availableSeats: number,       lockedSeats: number,       totalSeats: number  }As the database, we will be using MongoDB Atlas pay as you go service in the GCP marketplace After configuring the MongoDb instance, let’s create the transits function.gostep service init transits version \"0.1.0\" env nodejsThis will create a boilerplate NodeJs cloud function in {PROJECT_ROOT}/src/transits and it can be executed as a http request after the deployment.Now let’s include the dependencies.cd src/transitsnpm install --save mongodbAfter creating the transits database and the collection, we can add MongoDb connection URI and collection name in the src/trains/functions.json as an environment variable.\"environmentVariables\": {    \"DB_URI\": \"mongodb+srv://&lt;username&gt;:&lt;password&gt;@&lt;your-cluster-url&gt;/&lt;dbname&gt;\",    \"COLLECTION\": \"Transits\"    },First let’s use these environment variables and create a function to connect to the database.import { MongoClient } from \"mongodb\"; const DB_URI = process.env.DB_URI || \"&lt;Default DB con URI&gt;\"; const dbClient = new MongoClient(DB_URI); const initDbClientConnection = async () =&gt; {    try {        await dbClient.connect();    } catch(e) {        console.error(e);        throw new Error(\"Database failed to connect!\");    }};And now let’s write 2 functions to find transits documents and save/update documents.const COLLECTION = process.env.COLLECTION || \"Transits\";const query = async (queries) =&gt; {    try {        await initDbClientConnection();        const transits = dbClient.db().collection(COLLECTION);        return await transits.find(queries).toArray();    } catch (e) {        console.error(e);        throw new Error(\"Failed to query transits!\")    } finally {        await dbClient.close();    }}const save = async(transitId, patches) =&gt; {    try {        await initDbClientConnection();        const transits = dbClient.db().collection(COLLECTION);        const targetData = { \"$set\": patches };        await transits.updateOne({ transitId: transitId }, targetData, { upsert: true });    } catch(e) {        console.error(e);        throw new Error(\"Failed to update transits!\");    } finally {        await dbClient.close();    }}In the main function we map GET and PUT https methods to above functions.export const main = async (req, res) =&gt; {    if(req.method === \"GET\") {        res.json(await query(req.query));    } else if(req.method === \"PUT\") {        const transitId = req.query[\"transitId\"];        if(!transitId) {            res.status(400).send({ \"error\": \"Invalid parameters!\" })        }        await save(transitId, req.body);        res.status(201).send();    } else {        res.status(400).json({ \"error\": \"Invalid request\" });    }}Great! Now we have our transits service. We can deploy it by running below command in the project root,gostep deploy diffAfter the deployment, transits service can be executed using http requests. But the endpoint is not available for the public. To test it locally, use the bearer token which you can obtain using the Gcloud cli.gcloud auth print-identity-tokenReservations serviceNext, we are going to implement the entrypoint of the global transaction. Like before, let’s bootstrap a cloud function again. Run,gostep service init reservations version \"0.1.0\" env nodejsNow we have our boilerplate code in {PROJECT_ROOT}/src/reservations.Considering this scenario the reservations function is responsible for,  Get the user request via a HTTP request.  Call transits service and find out if there is any transit avaialable.  If a transit is avialable publish an message to the relavent topic.  Save the event data with it’s status as ‘IN_PROGRES’, to update later.We are going to keep the event data stored in a database. So that we can keep the status of the particular event to use later. For that purpose we use Google Cloud firestore(data store in native mode), which is a serverless easy to use document database.To enable Firestore run,gcloud firestore databases create --region=asia-southeast1After that let’s install the dependencies. In the function root({PROJECT_ROOT}/src/resrevations) run,npm install --save \"@google-cloud/firestore\" \"@google-cloud/pubsub\"Let’s assume below payload as the request JSON.{    \"day\": \"Monday\",    \"start\": \"Colombo\",    \"destination\": \"Ragama\",    \"numberOfSeats\": 10,    \"userId\": \"xyz@gmail.com\"}Once the user made his request we have to obtain the available transits for the requested day, start position and destination of the transit. To do that we will using a HTTP request to the transits service we implemeted before. Since the transits APIs are not publically available we have to use the google-auth-library to authorize requests from other services(See more). There is no need to add the auth library as a dependecy since it is an already included library in the cloud function runtime.First let’s add an environment variable for the transits API endpoint in {PROJECT_ROOT}/src/transits/function.json.    \"environmentVariables\": {        \"TRANSITS_API\": \"{HOST_ADDRESS}/transits\"    }After that let’s authorize our request to fetch available transits.import { GoogleAuth } from \"google-auth-library\";const TRANSITS_API = process.env.TRANSITS_API || \"{DEFAULT_TRANSITS_HOST_ADDRESS}/transits\";export const getAvailableTransits = async (numberOfSeats, day, destination, start) =&gt; {    try{        // Create an authorized client to invoke restricted Transits API.        const auth = new GoogleAuth();        const transitsApiClient = await auth.getIdTokenClient(TRANSITS_API);        const result = await transitsApiClient.request({            url: `${TRANSITS_API}?day=${day}&amp;destination=${destination}&amp;start=${start}`,            method: \"GET\"        });        return result.data.filter(element =&gt; element[\"availableSeats\"] &gt;= numberOfSeats);    } catch(e) {        console.error(e);    }};Based on the result of the API request, we proceed further. Let’s assume that we got a list of available transits and we selected the topmost transit. Now we will be saving the event data in firestore, with a unique Id(a generated UUID as correlationId) as the global transation Id to identify the local transactions group and the status of the current event. It will aid to identify the local transaction for later references.Same as before we can add the firestore collection name(reffered as kind in firestore) of the event as an environment varibale in {PROJECT_ROOT}/src/transits/function.json.    \"environmentVariables\": {        \"EVENT_DATA_COLLECTION\": \"reservations\"    }Now we can write our function to save event data in firestore. Please note that you don’t have to include configurations to authorize the connection to the firestore since the cloud function runtime has the authorized access to the firestore in the same project.import Firestore from \"@google-cloud/firestore\";const EVENT_DATA_COLLECTION = process.env.EVENT_DATA_COLLECTION || \"reservations\";export const saveEvent = async (id, eventData) =&gt; {    try {        const firestore = new Firestore();        const docRef = firestore.collection(EVENT_DATA_COLLECTION).doc(id);        await docRef.set(eventData, { merge: true });        const result = await docRef.get()        return result.exists? result.data(): {}; // return the updated doc for later references    } catch(e) {        console.error(e);        throw new Error(\"Error saving event data!\");    }};And since we have assumed that we have an available transit, we are going to publish a message to a pubsub topic to trigger the next event, bookings. First let’s create a topic for this purpose.gcloud pubsub topics create reservations.bookingsAnd please copy the output of that command and keep it saved, we are going to need it later. Same as before let’s have another environment varible for the topic name and wirte the function to publish the message. In the message we include correlationId, numberOfSeats, transitId and userId.import { PubSub } from \"@google-cloud/pubsub\";const BOOKINGS_TOPIC = process.env.BOOKINGS_TOPIC || \"reservations.bookings\";export const publishMessage = async (topic, message) =&gt; {    try {        const pubsubClient = new PubSub();        const dataBuffer = Buffer.from(JSON.stringify(message));        return await pubsubClient.topic(topic)            .publishMessage({ data: dataBuffer });    } catch (e) {        console.error(e);        throw new Error(`Error publishing message to ${topic}!`);    }}Now we have all helper functions and we can write the logic in the main funtion. Once the function is complted to deploy run,gostep deploy diffBookings serviceThe bookings function is responsible for hold the requested number of seats in selected transit until the global transaction is finished. The acting trigger of the function will be the reservations.bookings pubsub topic we create during the previous step. Once this service successfully locked the request number of seats it will publish a message to the relevenat pubsub topic to trigger the payments function.Let’s start. To initialize the function run,gostep service init bookings version \"0.1.0\" env nodejs trigger pubsubNow we have bootstrapped our cloud function in {PROJECT_ROOT/src/bookings. Let’s tell the funtion that it will triggered by the reservations.bookings topic. For that we can include the resource value we copied from the previous topic creation in the {PROJECT_ROOT/src/bookings/function.json.    \"eventTrigger\": {        \"eventType\": \"providers/cloud.pubsub/eventTypes/topic.publish\",        \"resource\": \"projects/{GCLOUD_PROJECT_ID}/topics/reservations.bookings\"    }Same as the reservations function, we need to save the local transaction’s event data with the correlationId and the status as IN_PROGRESS for later references. Also we can use same functions from the previous service to authorize requests to transits service and to publish the message to the next topic. What we can do is update the transit document to lock the requested number of seats.import { GoogleAuth } from \"google-auth-library\";const TRANSITS_API = process.env.TRANSITS_API || \"{TRANSITS_API_HOST}/transits\";export const getTransitsById = async (transitId, client) =&gt; {    try {        const result = await client.request({            method: 'GET',            url: `${TRANSITS_API}?transitId=${transitId}`        });        return result.length &gt; 0? result[0]: {};    } catch (e) {        console.error(e);        throw new Error(`Error fetching transit data: ${transitId}`);    }};export const updateTransitsById = async (id, newData, client) =&gt; {    try{        return await client.request({            method: \"PUT\",            url: `${TRANSITS_API}?transitId=${id}`,            body: newData        });    } catch(e) {        console.error(e);    }};export const main = async (eventData) =&gt; {    const transactionData = JSON.parse(atob(eventData.data)); // Extract data from pubsub message    const correlationId = transactionData[\"correlationId\"];    const numberOfSeats = Number(transactionData[\"numberOfSeats\"]);    const transitId = transactionData[\"transitId\"];    const userId = transactionData[\"userId\"];    const auth = new GoogleAuth();    const transitsApiClient = await auth.getIdTokenClient(TRANSITS_API);    const transit = await getTransitsById(transitId, transitsApiClient);    await updateTransitsById(transitId, {                \"lockedSeats\": transit[\"lockedSeats\"] + numberOfSeats,                \"availableSeats\": transit[\"availableSeats\"] - numberOfSeats                }, transitsApiClient);}And like before we will be creating the next pubsub topic to publish the message from bookings.gcloud pubsub topics create reservations.paymentsAfter a successful seat locking, we will be publishing a message with correlationId, numberOfClients and userId.Once the function has been completed we can deploy it using,gostep deploy diffGreat! Now we have covered common functionalities,  Consume HTTP requests  Function to function direct communication(via HTTP)  Read and update event data in firestore  Publishing and subscribing to Pubsub topicsThis is more than enough for us to implement next services. Therefor afterwards, I will be explaining the function’s role only.Payments serviceThe payments service will consume the message from reservations.payments and publish a message to reservations.bookingCompletions or reservations.bookingCancelletions accordingly for a successful payment or for a failed payment.Booking completions serviceThe booking completions will be consuming the messages from reservations.bookingCompletions topic, will be update the transit as the seat booking is completed and after that will update previously saved booking event’s status from IN_PROGRESS to COMPLETED for the relevant correlationId. Then the service will publish an message to the reservations.reservationCompletions topic.Booking cancellations serviceIn the event of a payment failure, after consuming the message from the topic reservations.bookingCancelletions this function will rollback the locked seats in the relevant transit, will update booking event’s status from IN_PROGRESS to FAILED for the relevant correlationId and will pass the correlationId to the reservations.reservationCancellations topic.Reservation completions serviceAs the final step of a completed series of events the reservation completions service will consume the correlation id for the transaction from reservations.reservationCompletions and will update previously saved reservations event’s status from IN_PROGRESS to COMPLETED. After that a message will be published to the reservations.notifications topic to send the successful transaction notifications to the customer.Reservation cancellations serviceConsuming the message from reservations.reservationCancellations this function will update previously saved reservations event’s status from IN_PROGRESS to FAILED and will publish a message to the reservations.notifications topic to send the failed transaction notifications status to the customer.Securing the entrypointAfter deploying all the services we can use Google API gateway to secure our reservations entrypoint of the transaction.Please refer API gatewey quickstart.🦖 Let’s look into Orchestration based solution in the next article."
  },
  
  {
    "title": "Super charge a Google cloud functions project",
    "url": "/posts/gostep-intro/",
    "categories": "Projects, DevOps",
    "tags": "gcp, serverless, gostep, python, cli",
    "date": "2020-06-19 00:00:00 +0530",
    





    
    "snippet": "When developing a microservices project with cloud functions, managing the cluster of functions all of them together can be a pain in the ass. That is why I thought of developing a simple cli tool ...",
    "content": "When developing a microservices project with cloud functions, managing the cluster of functions all of them together can be a pain in the ass. That is why I thought of developing a simple cli tool to super charge the development and deployment process.I named this little Pythonic tool as gostep a.k.a serverless templates provider for Google cloud platform. However this tool is still taking the baby steps. Hope to develop this to be more useful in future releases.I would like to show you how it works up to now.First of all…You need to have installed below components to use gostep cli.  Python version 3.x with PyPI a.k.a pip(https://www.python.org/download/releases/3.0/, https://pypi.org/project/pip/)  Gcloud sdk(https://cloud.google.com/sdk)  subversion(https://subversion.apache.org)  gostep(https://github.com/gostep-cli/gostep)Next steps…Now simply install gostep cli.pip install gostepAfter installing gostep, using gcloud sdk log in to your google cloud platform account.gcloud auth loginOnce you logged in, select the gcloud project that you want to use for your serverless functions.gcloud config set project {projectId}Oh! wait, to list down project Ids gcloud projects list or gostep gcloud projects can be used.All set…Now we are ready build a cloud functions cluster.First gostep needs a workspace directory, a gcloud service account and a credentials file for deployment purposes.We can initiate them by this command, gostep auth init {new_service_account_name} inside {workspace_directory}.gostep auth init my-service-account inside ./my-workspaceNow we can see a credentials file has been generated inside the workspace.Next we need to create a configuration file which keeps the project skeleton. We need to chose a default region for that. Otherwise gostep will choose that for us. To get a list for our gcloud project gostep gcloud locations can be used. Now we can simply do,gostep base init {project_name} location{gcloud_region_id} version \"0.1.0\" explains {description} inside {workspace_directory}In our case,cd my-workspacegostep base init my-new-project location asia-east2 version \"0.1.0\" explains \"my sample project\"It’s geen light now to create cloud functions now. gostep has specified template structures for this.Let’s simply bootstrap a python cloud function. For this purpose,gostep service init {cloud_function_name} location {gcloud_region_id} version {service_version} env {runtime} explains {desciption} inside {workspace_directory}Since we are in the workspace directory and we already set up a default location Id we won’t be using location and inside arguments.gostep service init my-python-function version \"0.1.0\" env pythonLet’s bootstrap another function with nodeJs.gostep service init my-nodejs-function version \"0.1.0\" env nodejsWe can see source files inside the {workspace_directory}/src. In our case inside, my-workspace/src/my-nodejs-function and my-workspace/src/my-python-function.Great…!Now our project is ready to get deployed.Aight… Let’s do,gostep deploy diffdiff keyword will only deploy the changes we made for our functions(gostep tracks md5 of the function directory). To deploy a single function it needs to be called by name. gostep deploy {function_name}. In our case,gostep deploy my-nodejs-functionBravo…!Now our functions are deployed and ready to be executed.In future releases…  More templates, templates for go lang, templates for Spring framework, etc…  Handle function triggers such as pubsub, events, etc…  Run cloud functions cluster in local environment, so developers can benefit debugging.Please find the source code in https://github.com/gostep-cli/gostep."
  },
  
  {
    "title": "Application Deployment in Apache Tomcat on GCE Using Ansible",
    "url": "/posts/tomcat-gce-ansible-demo/",
    "categories": "Posts, DevOps",
    "tags": "tomcat, gce, ansible, configuration-management",
    "date": "2018-03-01 00:00:00 +0530",
    





    
    "snippet": "Think about a person who needs a cloud instance temporarily to deploy a web application to do tests frequently and throughout the time he deploys the application,use it for a while and then deletes...",
    "content": "Think about a person who needs a cloud instance temporarily to deploy a web application to do tests frequently and throughout the time he deploys the application,use it for a while and then deletes the instance to save the cost.Or someone needs to create a cluster, thus he needs to instantiate several cloud servers at once,install dependencies and deploy the application on each server. Doing these tasks by hand costs much effort and it is inefficient.The way to make such scenarios easier, efficient and effective is making a reusable structure which does these repetitive tasks when we invoke it.For that purpose, we use configuration management.This tutorial is about a such scenario, to splash the easiness of using a reusable code base when deploying an application,using Ansible(a radical and impressive configuration management tool with its capabilities and ease of use compared to other tools),Google cloud platform(future potential cloud services with a good pricing model) and Apache Tomcat(one of the most popular web servers in the Java community). For this purpose, we are going to use the gce-tomcat-ansible-demo repository, a concatenation of Ansible playbooks(the reusable code base) implemented by me to reflect this task.  Running this will create a Google compute engine in a given Google cloud platform project, install java,   configure an Apache Tomcat server and will deploy a war file according to given metadata.   (To understand playbooks knowing Ansible basics is more than enough. Refer Ansible documentation)Getting startedTo run the playbook you need to have a Google cloud platform account, a Google cloud platform project and a service accountto manipulate Google cloud project with appropriate roles and permission and an Ansible running machine.There are several ways to install Ansible but here let’s install it using python pip package manager since the installationis not going to depend on which operating system you use. But first make sure Python version 2,Python development and Python pip(most probably python-dev and python-pip respectively,refer installing pip with package managers for more information) packages have been installed on the machine that you are going to install Ansible. To make sure, try running the command below.pip listAnd then clone the repository gce-tomcat-ansible-demo, which contains playbooks to create a Google compute engine instance, install Java, configure a Apache Tomcat server and deploy a given war file on the configured application server.git clone https://github.com/lpsandaruwan/gce-tomcat-ansible-demo.gitThen change the current working directory into the cloned repository.cd gce-tomcat-ansible-demoNow use the requirements.txt to install appropriate Python pip package versions which this playbook has been written and tested for.pip install -r requirements.txtThis will install ansible and apache-libcloud(a fine interface to deal with popular cloud services)Python packages which we are going to use for manipulating Google cloud project.Make sure that you have a working Google cloud platform project(if not refer creating and manage projects),and a service account assigned to it(do not use the default service account since it has full permission over the project,refer service accounts for more information) and then obtain the project ID,private JSON keyfile(you can obtain the JSON key file when creating a new service account,if not refer service account credentials) and the service account email from them.Now we have to configure Ansible running machine to access the GCP project.Here Let’s use Google cloud SDK to make things easier(refer install Google cloud SDK).After installing the SDK run the below command to initialize.gcloud initAnd it will direct you to the web page in your browser. From there allow the access to the SDK. Now in the terminal select the appropriate project ID. After that you should be able to run playbooks on the appropriate project and manipulate it.  If you run into a permission problem connecting the instance configure SSH authentication using cloud SDK tools by running the command below.  (refer gcloud compute config-ssh)gcloud compute config-sshPlay itNow configure the file gce-vars/authentication and update the obtained metadata from GCP project and service account in playbook.project_id: “project-id-193706”credentials_file: “/path/to/private/json/key/file”service_account_email: “tomcat-ansible-demo@service-account-193706.iam.gserviceaccount.com”After that change instance metadata in gce-vars/instance as you need. Here,we are going to add firewall rules to allow HTTP traffic on 8080 ports for Apache Tomcat servername: tomcat-ansible-demotype: f1-microimage: debian-9zone: europe-west1-ballowed_ports_tcp: tcp:8080allowed_ports_udp: udp:8080Then set the ANSIBLE_HOSTS environment variable required by Ansible for SSH interactions.To do that simply put hostnames in hosts file and import it as below.export ANSIBLE_HOSTS=hostsNow you should be able to run this project. Simply run the main playbook.ansible-playbook run.ymFinal output will be as below. And after a successful run you will have your application deployed in anApache Tomcat server on a Google compute engine instance.Appendix(If you wish to change Java version, Tomcat version etc.configure main.yml in defaults directory in roles. They contain configuration variables with lower priorities.)gce-tomcat-ansible-demo|------gce_vars\t\t\t\t# variables related to Google cloud platform|\t|\tauthentication\t\t# Google project and service account related metadata|\t|\tinstance\t\t# GCE instance related metadata||-------roles|\t|-------java\t\t\t\t# role to install Java|\t|\t|-------defaults|\t|\t|\t|\tmain.yml\t# default variables for java role|\t|\t||\t|\t|-------tasks|\t|\t|\t|\tmain.yml\t# tasks to download and install Java|\t||\t|-------tomcat|\t|\t|-------defaults|\t|\t|\t|\tmain.yml\t# default variables for tomcat role|\t|\t||\t|\t|-------files|\t|\t|\t|\ttomcat-users.xml\t# set tomcat manager credentials|\t|\t||\t|\t|-------tasks|\t|\t|\t|\tmain.yml\t# tasks to download and configure tomcat|\t||\t|-------tomcat-deploy|\t|\t|-------defaults|\t|\t|\t|\tmain.yml\t# default variables for tomcat-deploy role|\t|\t||\t|\t|-------tasks|\t|\t|\t|\tmain.yml\t# tasks to deploy the given war file||\thosts\t\t\t\t\t# ansible hosts|\tbootstrap-instance.yml\t\t\t# playbook to initiate google cloud instance|\tdeploy-war.yml\t\t\t\t# playbook to deploy war file|\tinstall-java.yml\t\t\t# playbook to install Java|\tinstall-tomcat.yml\t\t\t# playbook to install Apache Tomcat|\trun.yml\t\t\t\t\t# main playbook to rungce-tomcat-ansible-demo post by Lahiru Pathirage is licensed under a Creative Commons Attribution 4.0 International License.Based on a work at https://github.com/lpsandaruwan/gce-tomcat-ansible-demo."
  },
  
  {
    "title": "Setting up Creative Labs USB DAC volume knob on Linux",
    "url": "/posts/sb1095-volume-knob-linux/",
    "categories": "Posts, Linux",
    "tags": "usb dac, creative labs, sb1095, linux, volume knob",
    "date": "2017-05-11 00:00:00 +0530",
    





    
    "snippet": "Lately I bought a Creative Labs SB1095, a 5.1 USB DAC for my laptop.This USB sound card works perfectly on Linux.The sound quality is better than the integrated,but it has a volume knob on it and a...",
    "content": "Lately I bought a Creative Labs SB1095, a 5.1 USB DAC for my laptop.This USB sound card works perfectly on Linux.The sound quality is better than the integrated,but it has a volume knob on it and a remote controller,which does not support out of the box by Linux distributions which I have tried(Ubuntu, Linux Mint, OpenSuse, Arch Linux).After digging up the internet a little bit I perceived that it is required to configure a software volume controller to handle the volume knob.However, I did not want to go for that kind of advanced configurations, and finally found an easy workaround for this purpose. Would like to take down the steps,so it might help others which have the same issue.To achieve this I used lirc (an application which interprets IR actions) to detect volume knob and remote actions and wrapped it with irexec (trigger actions for lirc inputs) to change system sound volume.I am currently using Linux Mint 18.1, so this solution will work perfectly with Ubuntu 16.04 derivatives.For other Linux distributions please follow this guide.RequirementsFirst I installed these packages, selected Creative USB IR Receiver (SB0540) for remote controller configuration(not the SB1095,but it has the same configurations) and selected none for IR transmitter in appearing configuration menus when installing.sudo apt install lirc lirc-xConfigurationsThen I changed the REMOTE_DRIVER in lirc hardware configuration file /etc/lirc/hardware.conf, left other settings unchanged.# ~/.lircrcbegin remote = * prog = irexec config = amixer -D pulse sset Master 5%- button = vol- repeat = 1endbegin remote = * prog = irexec config = amixer -D pulse sset Master 5%+ button = vol+ repeat = 1endbegin remote = * prog = irexec config = amixer -D pulse sset Master 0 button = muteendTestThen I restarted lirc daemon and loaded irexec daemon.sudo service lirc restartirexec -d # make a startup entry to load on system boot upNow I have the volume knob working just fine.If the volume is not changing check whether lirc detects inputs by the USB device using the command irw# irw sample output for volume knob changes0000000000000010 01 vol+ RM-15000000000000000010 00 vol+ RM-15000000000000000010 01 vol+ RM-1500000000000000000f 03 vol- RM-1500000000000000000f 00 vol- RM-1500000000000000000d 00 mute RM-1500If the output is something like the above, try changing config values(commands to change sound volume) in the ~/.lircrc after that try restarting lirc daemon and loading irexec again.Sourceshttp://alsa.opensrc.org/Usb-audio#Creative_USB_X-Fi_Surround_5.1http://www.lirc.org/html/configure.html"
  },
  
  {
    "title": "Continuous Code Quality On My OpenSource Project",
    "url": "/posts/github-sonarqube/",
    "categories": "Posts, DevOps",
    "tags": "code-quality, github, travis-ci, sonarqube",
    "date": "2017-04-20 00:00:00 +0530",
    





    
    "snippet": "Good quality in code plays an essential role when it comes to software,thus it assets efficiency, reliability, robustness, portability, maintainability and readability like essential factors.Consid...",
    "content": "Good quality in code plays an essential role when it comes to software,thus it assets efficiency, reliability, robustness, portability, maintainability and readability like essential factors.Considering a GitHub project, there are plenty of options to measure code quality.Considering options I would like to chose SonarQube for this particular purpose.Let me take down the steps, how I used SonarQube to measure code quality using a Java project, one of my GitHub hosted projects, Depli.NB: The best way to analyze a maven project is to use the maven sonar plugin as the SonarQube docs says.You do not require a sonar-project.properties in that case.Step 1 - Create an account in travis-ci.orgSonarQube needs sonar-runner to analyze the code.To run the analysis process using sonar-runner on code changes continuously, the ideal solution is using a CI server.Here I had to use Travis-CI since it is the perfect matured CI solution for GitHub projects.I created and logged into Travis using my GitHub account and activated it for my repository.Step 2 - Create an account in sonarqube.comThen I created and logged into SonarQube using the same GitHub account.Step 3 - Create Travis-CI configuration fileNext step was to create a configuration file for Travis-CI to instruct it to how to run the sonar-scanner as known as sonar-runner.To do that I created a .travis.yml, A YAML file in my project’s root directory.dist: trusty # chose ubuntu trusty as the workersudo: requiredaddons:  sonarqube:    organization: lpsandaruwan-github # organization token from https://sonarqube.com/account/organizationsjdk:- oraclejdk8script:- mvn clean install -DskipTests # skipped tests because I have not written.- sonar-scanner # tell travis to run sonar scannercache:  directories:  - \"$HOME/.sonar/cache\"step 4 - Create a SonarQube tokenAfter creating a Travis configuration file I generated a security token and copied it to clipboard,for Travis to use when updating SonarQube database.Step 5 - Encrypt SonarQube tokenPublic access to a security token is a bad thing. So I had to encrypt the SonarQube token when inserting it to Travis configuration file.To achieve that I used travis from ruby gems.cd /path/to/my/project/roottravis encrypt MY_SONARQUBE_TOKEN --add addons.sonarqube.tokenStep 6 - Create a SonarQube configuration fileA metadata file including project details is required for SonarQube. So I created sonar-project.properties in my project’s root.Here sonar.sources is the place where sonar-scanner starts to analyze.sonar.projectKey=com.sonarqube.lpsandaruwan.deplisonar.projectName=Depli - JVM Monitoring Dashboardsonar.projectVersion=0.2.0-SNAPSHOTsonar.links.homepage=https://lahirus.com/deplisonar.links.ci=https://travis-ci.org/lpsandaruwan/deplisonar.links.scm=https://github.com/lpsandaruwan/deplisonar.links.issue=https://github.com/lpsandaruwan/depli/issuessonar.sources=src/mainStep 7 - Add status to read meNow to display the project status on readme, I added labels from Travis and SonarQube on README.md file.[![Build Status](https://travis-ci.org/USERNAME/PROJECT_NAME.png)](https://travis-ci.org/USERNAME/PROJECT_NAME)[![Quality Gate](https://sonarqube.com/api/badges/gate?key=SONAR_PROJECT_KEY)](https://sonarqube.com/dashboard/index/SONAR_PROJECT_KEY)Bravo!After following above steps I pushed all changes to GitHub. And waited until Travis sent me a mail confirming that my build has been successful.Now the readme is displaying the build status and whether my project has passed the quality gate.By clicking on the quality gate badge, I can access the SonarQube dashboard, detailed analysis of code quality of my repository.Please refer my open source project, https://github.com/lpsandaruwan/depli if there is any doubt."
  },
  
  {
    "title": "Continuous listening to remote text files using python",
    "url": "/posts/log-tracker/",
    "categories": "Projects, DevOps",
    "tags": "python, log files, paramiko, wrapper application",
    "date": "2017-04-14 00:00:00 +0530",
    





    
    "snippet": "Log Tracker is a simple wrapper around Python paramiko to track text files using SSH.It gives you the ability to create custom python functions to track and analyze log files the way you want.A use...",
    "content": "Log Tracker is a simple wrapper around Python paramiko to track text files using SSH.It gives you the ability to create custom python functions to track and analyze log files the way you want.A user can access the contents in multiple log files at the same time also.Using a custom function a user can display a log content in a web interface using Flask like lightweight web service,so then anyone can analyze contents easily, without wasting time to login into servers and download contents.For more information please refer below links.Website: https://lahirus.com/log-trackerWiki: https://github.com/lpsandaruwan/log-tracker/wikiLicense: GPLv3 or laterSource code: https://github.com/lpsandaruwan/log-trackerReleases: https://github.com/lpsandaruwan/log-tracker/releases"
  },
  
  {
    "title": "Depli - A JVM monitor application",
    "url": "/posts/depli/",
    "categories": "Projects, DevOps",
    "tags": "java, spring-boot, angularjs, jvm performance",
    "date": "2017-04-03 00:00:00 +0530",
    





    
    "snippet": "Depli provides you the 10-second solution for monitoring JVMs. Just add a JMX remote connection using webUI and see how it works.Depli provides you a rich UI, you can even search for running thread...",
    "content": "Depli provides you the 10-second solution for monitoring JVMs. Just add a JMX remote connection using webUI and see how it works.Depli provides you a rich UI, you can even search for running threads, classpaths etc.This handsome tool has been released under GPL license on GitHub.Website: https://lahirus.com/depliWiki: https://github.com/lpsandaruwan/depli/wikiLicense: GPLv3 or laterSource code: https://github.com/lpsandaruwan/depliReleases: https://github.com/lpsandaruwan/depli/releases"
  },
  
  {
    "title": "JVM CPU usage using Java MXBeans",
    "url": "/posts/jvm-cpu-usage/",
    "categories": "Posts, Other",
    "tags": "ava, jvm cpu utilization, mxbeans, devops",
    "date": "2017-02-26 00:00:00 +0530",
    





    
    "snippet": "This is a solution to a problem, occurred to me while developing Depli a JVM monitoring dashboard which uses JMX remote connections.There is no way to get the JVM CPU usage directly using MXBeans i...",
    "content": "This is a solution to a problem, occurred to me while developing Depli a JVM monitoring dashboard which uses JMX remote connections.There is no way to get the JVM CPU usage directly using MXBeans in JDKs older than version 7. For my application I wanted a universal method.Finally, I got it working thanks to source code of jconsole.ExplanationIdeal solution to calculate CPU usage is periodically look at the idle time and get the time that JVM is not idle.But there is no method to expose idle time in MXBeans. So here, JVM CPU usage is calculated using getting theratio of how much discrete time slices JVM used and how long JVM was up while using those time slices(total JVM uptime in all the available threads for JVM),in a particular time period. Below algorithm will explain it better.previousJvmProcessCpuTime = 0;previousJvmUptime = 0;function getCpuUsagePercentage() {    elapsedJvmCpuTime = currentJvmCputime - previousJvmProcessCpuTime;    elapsedJvmUptime = currentJvmUptime - previousJvmUptime;    // total jvm uptime    totalElapsedJvmUptime = elapsedJvmUptime * availableNumberOfCpusForJvm;    // calculate cpu usage ratio    cpuUsage = (elapsedJvmCpuTime / totalElapsedJvmUptime);    previousJvmProcessCpuTime = currentJvmCputime;    previousJvmUptime = currentJvmUptime;    // return as a percentage    return cpuUsage * 100;}Get it workingWe will be using remote method invocation(RMI) to connect and call methods in remote MXBean interfaces.First we have to connect to a JMX remote connection and have to initiate a managed beans server connection.// hardcoded connection parametersfinal String HOSTNAME = \"localhost\";final int PORT = 9999;// initiate address of the JMX API connector serverString serviceURL = \"service:jmx:rmi:///jndi/rmi://\" + HOSTNAME + \":\" + PORT + \"/jmxrmi\";JMXServiceURL jmxServiceURL = new JMXServiceURL(serviceURL);// initiate client side JMX API connector// here we set environment attributes to null, since I am not using any authentication method connect to JMX remoteJMXConnector jmxConnector = JMXConnectorFactory.connect(jmxServiceURL, null);// initiate managed bean server connectionMBeanServerConnection mBeanServerConnection = jmxConnector.getMBeanServerConnection();Now we can create proxy connections to MXBean interfaces to forward method calls though the managed bean serverconnection that we have created above. We will be using runtime MXBean, operating system MXBean and operating systemMXBean from platform extension.com.sun.management.OperatingSystemMXBean peOperatingSystemMXBean;java.lang.management.OperatingSystemMXBean operatingSystemMXBean;java.lang.management.RuntimeMXBean runtimeMXBean;// initiate proxy connectionspeOperatingSystemMXBean = ManagementFactory.newPlatformMXBeanProxy(        mBeanServerConnection,        ManagementFactory.OPERATING_SYSTEM_MXBEAN_NAME,        com.sun.management.OperatingSystemMXBean.class);operatingSystemMXBean = ManagementFactory.newPlatformMXBeanProxy(        mBeanServerConnection,        ManagementFactory.OPERATING_SYSTEM_MXBEAN_NAME,        OperatingSystemMXBean.class);runtimeMXBean = ManagementFactory.newPlatformMXBeanProxy(        mBeanServerConnection,        ManagementFactory.RUNTIME_MXBEAN_NAME,        RuntimeMXBean.class);We can call the appropriate methods to get required data now. We need data on how long JVM processes used the CPU, JVM uptime andhow much processors allowed for the JVM in a paticular time period.// keeping previous timestamplong previousJvmProcessCpuTime = 0;long previousJvmUptime = 0;// Get JVM CPU usagepublic float getJvmCpuUsage() {    // elapsed process time is in nanoseconds    long elapsedProcessCpuTime = peOperatingSystemMXBean.getProcessCpuTime() - previousJvmProcessCpuTime;    // elapsed uptime is in milliseconds    long elapsedJvmUptime = runtimeMXBean.getUptime() - previousJvmUptime;    // total jvm uptime on all the available processors    long totalElapsedJvmUptime = elapsedJvmUptime * operatingSystemMXBean.getAvailableProcessors();        // calculate cpu usage as a percentage value    // to convert nanoseconds to milliseconds divide it by 1000000 and to get a percentage multiply it by 100    float cpuUsage = elapsedProcessCpuTime / (totalElapsedJvmUptime * 10000F);    // set old timestamp values    previousJvmProcessCpuTime = peOperatingSystemMXBean.getProcessCpuTime();    previousJvmUptime = runtimeMXBean.getUptime();    return cpuUsage;}Now we can get JVM CPU usage by calling the getJvmCpuUsage periodically. You can obtain the complete Java code from this Gist."
  },
  
  {
    "title": "Savior ship - A cross flatform C++ game",
    "url": "/posts/savior-ship/",
    "categories": "Projects, Gaming",
    "tags": "c++, sdl2, game, 2d",
    "date": "2017-02-23 00:00:00 +0530",
    





    
    "snippet": "Savior Ship is a simple 2D shooter game implemented using C++ and simple direct media layer version 2(SDL2),This is the way I attempted to refresh my skills on C++. Savior ship has been released un...",
    "content": "Savior Ship is a simple 2D shooter game implemented using C++ and simple direct media layer version 2(SDL2),This is the way I attempted to refresh my skills on C++. Savior ship has been released under GPL license in GitHub for the purposeof helping out the people who seek SDL2 beginner level applications to learn.Website: https://lahirus.com/savior-shipLicense: GPLv3 or laterSource code: https://github.com/lpsandaruwan/savior-shipReleases: https://github.com/lpsandaruwan/savior-ship/releases"
  },
  
  {
    "title": "Surgeon Tux - A Jekyll template",
    "url": "/posts/surgeon-tux/",
    "categories": "Projects, Other",
    "tags": "jekyll, template, monospace, dark",
    "date": "2017-02-23 00:00:00 +0530",
    





    
    "snippet": "Surgeon Tux is a GPL(v3) licensed free Jekyll template meant for terminal lovers.I tried my best to give it the appearance of a terminal.Where can I find the source code?Surgeon Tux code is hosted ...",
    "content": "Surgeon Tux is a GPL(v3) licensed free Jekyll template meant for terminal lovers.I tried my best to give it the appearance of a terminal.Where can I find the source code?Surgeon Tux code is hosted in a github repository, you can obtain it from https://github.com/lpsandaruwan/surgeon-tuxHow to use it?First clone the surgeon tux,git clone https://github.com/lpsandaruwan/surgeon-tux.gitcd surgeon-tuxMake sure you have installed RubyGems package manager,gem install jekyllgem install jekyll-paginateBefore running Jekyll, consider changing the configurations listed in _config.ymlTo run Surgeon Tux,jekyll serveNow log into http://localhost:4000 from your web browser.You can refer the demo here, https://lahirus.com/surgeon-tux"
  },
  
  {
    "title": "Traverse through font design timeline",
    "url": "/posts/kalagola/",
    "categories": "Projects, Other",
    "tags": "python, cli, fonts",
    "date": "2017-01-26 00:00:00 +0530",
    





    
    "snippet": "KalaGola is a cross platform Pythonic tool to traverse through the timeline of your GitHub hosted font repository,and generate a video file to show how your font evolved through time. Just provide ...",
    "content": "KalaGola is a cross platform Pythonic tool to traverse through the timeline of your GitHub hosted font repository,and generate a video file to show how your font evolved through time. Just provide your custom web template andrepository information as a YAML file or as command line arguments, KalaGola will serve you your video.How does it work?KalaGola uses weasyprint to capture html snapshots after checking out each and every commit made on a specificfont file and copying the font file on that state to the assets directory which html template uses. As soon as possible KalaGola finished capturing images,using OpenCV bindings for Python, KalaGola converts captured images to frames and then frames to a video file.How can I use it?Like mentioned above, KalaGola configurations can be given as a YAML file or as command line arguments. If no argument has been providedKalaGola uses config.yml file by default.Contents of config.yml filename: myfontassets_dir: template/assetsbranch: gh-pagesfont_file: fonts/AbhayaLibre-Regular.otfindex_file: template/index.htmlinterval: 100repository: abhaya-libre-fontstylesheet: template/assets/stylesheet.cssuser: mooniak      name - For name, provide any name you like(do not use spaces), once KalaGola has finished, you will find name.avi video file in the videos directory from KalaGola  home.        assets_dir - This is where you are going put your font while traversing commits. The font file copied to assets directory will be named as myFont and you shoulduse it in your stylesheet like this or in a better way,  @font-face {    font-family: \"My Font\";    src: url('assets/myfont') format(\"woff\");}      index_file - This is the template html file which contains the layout designs, designed using your font file, which is to be captured while changing the state of font file.        stylesheet - The stylesheet can be embedded into html file’s header or YAML file’s stylesheet field.        repository - This one stands for GitHub repository name.        user - Consider puttting GitHub user name or organization here.        branch - This is the branch of the repository which contains font files, which is affected by continuous integration process, where font files, ttf or otf files have been automatically built or updated, or simply have been built manually and pushed.        font_file - This is the path to the font file in the branch, which KalaGola refers to load the templates and capture images.        interval - This field is used to reduce the CPU utilization while KalaGola capturing snapshot, by default it is 100, which means for every 100 of captures,KalaGola sleeps for a second. For older PCs reduce the interval number so they won’t get heat up.  To get an idea about the project structure, please refer Abhaya Libre Font, which is mentioned in the default config.yml file of KalaGola.Command line argumentsSimply use command line arguments to override default settings in runtime. They are the same as the YAML config file’s options.Prefer kalagola -h for more information,usage: kalagola.py [-h] [-a ADIR] [-b BRANCH] [-f FONT] [-i INDEX] [-n NAME]                   [-r REPO] [-s STYLESHEET] [-t INTERVAL] [-u USER] [-y YAML]optional arguments:  -h, --help            show this help message and exit  -a ADIR, --adir ADIR  Directory to put font file dynamically  -b BRANCH, --branch BRANCH                        Branch containing font files  -f FONT, --font FONT  Font file source  -i INDEX, --index INDEX                        Custom index html file  -n NAME, --name NAME  Output file name  -r REPO, --repo REPO  GitHub user repository  -s STYLESHEET, --stylesheet STYLESHEET                        Custom CSS style sheet  -t INTERVAL, --interval INTERVAL                        Refresh intervals to increase CPU usage  -u USER, --user USER  GitHub username/organization name  -y YAML, --yaml YAML  Read settings from yaml fileHow can I get KalaGola?Download KalaGola releases from here, https://github.com/lpsandaruwan/kalagola/releasesYou also can obtain the source code from here, https://github.com/lpsandaruwan/kalagolaI have an issue. What can I do?Please feel free to report issues here, https://github.com/pathumego/kalagola/issuesCredits and contributors  Pathum Egodawatta  Lahiru PathirageLicenseKalaGola is relased under GPLv3. Anyone is permitted to do copy, edit or redistribute."
  }
  
]

